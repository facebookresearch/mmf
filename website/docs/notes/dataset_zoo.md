---
id: dataset_zoo
title: Dataset Zoo
sidebar_label: Dataset Zoo
---

Here is the current list of datasets currently supported in MMF:

- **CLEVR** [Paper : CLEVR A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning][arxiv](https://arxiv.org/abs/1612.06890)]
- **COCO Captions** [Paper : Microsoft COCO Captions: Data Collection and Evaluation Server][arxiv](https://arxiv.org/abs/1504.00325)]
- **Conceptual Captions** [Paper : Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning][website](https://ai.google.com/research/ConceptualCaptions)]
- **Hateful Memes** [Paper : The Hateful Memes Challenge: Detecting Hate Speech in Multimodal Memes][arxiv](https://arxiv.org/abs/2005.04790)]
- **MM IMDB** [Paper : Gated Multimodal Units for Information Fusion][arxiv](https://arxiv.org/abs/1702.01992)] [[website](http://lisi1.unal.edu.co/mmimdb)]
- **NLVR2** [Paper : A Corpus for Reasoning About Natural Language Grounded in Photographs][arxiv](https://arxiv.org/abs/1811.00491)] [[website](http://lil.nlp.cornell.edu/nlvr/)]
- **OCRVQA** [Paper : OCR-VQA: Visual Question Answering by Reading Text in Images][website](https://ocr-vqa.github.io/)]
- **STVQA** [Paper : Scene Text Visual Question Answering][arxiv](https://arxiv.org/abs/1905.13648)]
- **TextVQA** [Paper : Towards VQA Models That Can Read][arxiv](https://arxiv.org/abs/1904.08920)] [[website](https://textvqa.org/)]
- **TextCaps** [Paper : TextCaps: a Dataset for Image Captioning with Reading Comprehension][arxiv](https://arxiv.org/abs/2003.12462)]
- **Visual Dialog** [Paper : Visual Dialog][website](https://visualdialog.org/)]
- **SNLI-VE** [Paper : Visual Entailment: A Novel Task for Fine-Grained Image Understanding][arxiv](https://arxiv.org/abs/1901.06706)] [[website](https://github.com/necla-ml/SNLI-VE)]
- **Visual Genome** [Paper : Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations][arxiv](https://arxiv.org/abs/1602.07332)] [[website](https://visualgenome.org/)]
- **VizWiZ** [Paper : VizWiz Grand Challenge: Answering Visual Questions from Blind People][arxiv](https://arxiv.org/abs/1802.08218)] [[website](https://vizwiz.org/)]
- **VQA2.0** [Paper : VQA: Visual Question Answering][arxiv](https://arxiv.org/abs/1505.00468)] [[website](https://visualqa.org/)]
