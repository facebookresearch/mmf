{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of KDD_Tutorial",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RecgHyhTKc8y",
        "colab_type": "text"
      },
      "source": [
        "# KDD Tutorial\n",
        "# Using MMF for knowledge based recommender systems\n",
        "\n",
        "In this tutorial, we will show how we can use MMF to build a knowledge based recommender system. We will first show how we can setup a dataset inside MMF which in this case is [OKVQA](https://okvqa.allenai.org/) dataset. Then, we will dive deeper into how we can create a Multimodal Transformer which can answer knowledge based questions in OKVQA to a reasonable accuracy compared to the state-of-the-art."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLXBzxjdXZmh",
        "colab_type": "text"
      },
      "source": [
        "### Prerequisites \n",
        "Please enable GPU in this notebook: Runtime > Change runtime type > Hardware Accelerator > Set to GPU\n",
        "\n",
        "Throughout the tutorial, if you face any error related to a module not found, please restart your runtime."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdrS6pKJHjKp",
        "colab_type": "text"
      },
      "source": [
        "# Install MMF and Dependencies\n",
        "\n",
        "This involves cloning the MMF repo, installing it in develop mode and setting up the right versions of the dependencies already present in the colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ll23oWra4azM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Enabled auto-reload\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_ekOA-_Gnus",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cd /content && git clone https://github.com/facebookresearch/mmf.git "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9rVQE2qGq_g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install torch==1.5.0+cu101 torchvision==0.6.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!git checkout 95e89ee\n",
        "!python --version && cd mmf && python setup.py develop\n",
        "!pip install --upgrade pyyaml\n",
        "!pip install --upgrade GitPython"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QehU-tNaTQ2g",
        "colab_type": "text"
      },
      "source": [
        "## Add MMF to PATH so that it is discoverable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBu-8TRQTISB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/mmf\")\n",
        "\n",
        "from mmf.utils.env import setup_imports\n",
        "\n",
        "setup_imports()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OVjhj4cHvcJ",
        "colab_type": "text"
      },
      "source": [
        "# Dataset\n",
        "\n",
        "To create a dataset, there are three components:\n",
        "1. An annotation database to load the annotation files provided by the dataset.\n",
        "2. An image database which is provided by MMF and can be overridden if needed for specific use cases.\n",
        "3. A builder class that is registered to MMF and returns back an instance of torch.utils.data.Dataset and will be used by MMF to load data which will be passed to the model.\n",
        "\n",
        "Let's dive into details of each of this components."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoKcD869_HQ7",
        "colab_type": "text"
      },
      "source": [
        "### OKVQA Dataset\n",
        "\n",
        "First, let's check how does the OKVQA dataset actually look like so that we can understand how do we have to create an annotation database based on this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3n-aoLeKNHKV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -q https://okvqa.allenai.org/static/data/mscoco_val2014_annotations.json.zip && unzip -o mscoco_val2014_annotations.json.zip "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdzZEadNNdqS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -q https://okvqa.allenai.org/static/data/OpenEnded_mscoco_val2014_questions.json.zip && unzip -o OpenEnded_mscoco_val2014_questions.json.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCvWFxy5NjyH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "with open(\"mscoco_val2014_annotations.json\") as f:\n",
        "  annotations = json.load(f)\n",
        "with open(\"OpenEnded_mscoco_val2014_questions.json\") as f:\n",
        "  questions = json.load(f)\n",
        "\n",
        "print(\"Annotation keys: \", annotations.keys())\n",
        "print(\"Annotation: \", annotations[\"annotations\"][0])\n",
        "print(\"Question keys: \", questions.keys())\n",
        "print(\"Question: \", questions[\"questions\"][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMoBgTHuCGt7",
        "colab_type": "text"
      },
      "source": [
        "## Creating the annotation database\n",
        "\n",
        "The annotation database loads annotations from the data provided by the dataset and sets it to data variable. `self.data` should be list where an index can be passed to get a corresponding unique data same as how it works with torch.utils.dataset.Dataset.\n",
        "\n",
        "We will override `load_annotation_db` method here to load our custom data. MMF's default `AnnotationDatabase` does handle a lot of common cases on its own, but if your data doesn't fit any of them, like in this case, create your own.\n",
        "\n",
        "**Note:** If next command fails, please restart the runtime from \"Runtime > Restart runtime\" and run the command again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxEvZHqATt1H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "\n",
        "from mmf.datasets.databases.annotation_database import AnnotationDatabase\n",
        "\n",
        "## Inherit default MMF's AnnotationDatabase and update necessary\n",
        "## function `load_annotation_db`\n",
        "class OKVQAAnnotationDatabase(AnnotationDatabase):\n",
        "  def load_annotation_db(self, path: str):\n",
        "    # Expect two paths, one to questions and one to annotations\n",
        "    path = path.split(\",\")\n",
        "    assert len(path) == 2, (\n",
        "        \"OKVQA requires 2 paths; one to questions and one to annotations\"\n",
        "    )\n",
        "\n",
        "    with open(path[0]) as f:\n",
        "      path_0 = json.load(f)\n",
        "    with open(path[1]) as f:\n",
        "      path_1 = json.load(f)\n",
        "\n",
        "    if \"annotations\" in path_0:\n",
        "      annotations = path_0\n",
        "      questions = path_1\n",
        "    else:\n",
        "      annotations = path_1\n",
        "      questions = path_0\n",
        "    \n",
        "    # Convert to linear format\n",
        "    data = []\n",
        "    question_dict = {}\n",
        "    for question in questions[\"questions\"]:\n",
        "      question_dict[question[\"question_id\"]] = question[\"question\"]\n",
        "\n",
        "    for annotation in annotations[\"annotations\"]:\n",
        "      annotation[\"question\"] = question_dict[annotation[\"question_id\"]]\n",
        "      answers = []\n",
        "      for answer in annotation[\"answers\"]:\n",
        "        answers.append(answer[\"answer\"])\n",
        "      annotation[\"answers\"] = answers\n",
        "      data.append(annotation)\n",
        "    \n",
        "    self.data = data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtLIvh5aPWSN",
        "colab_type": "text"
      },
      "source": [
        "### Building the dataset\n",
        "\n",
        "Now, we will build the OKVQA dataset object that follows the syntax same as\n",
        "torch.utils.data.Dataset. We will implement the `__getitem__` function and `__len__` is already implemented in the MMFDataset which equals to length of the annotation database.\n",
        "\n",
        "To use our custom annotation database, we will override the `build_annotation_db` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wu5o2DbhHp8M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from typing import Dict\n",
        "\n",
        "from mmf.common.sample import Sample\n",
        "from mmf.datasets.mmf_dataset import MMFDataset\n",
        "from mmf.utils.visualize import visualize_images\n",
        "\n",
        "\n",
        "class OKVQADataset(MMFDataset):\n",
        "  def __init__(self, config: Dict, dataset_type: str, index, *args, **kwargs):\n",
        "    # The super call will build annotation database, image database and \n",
        "    # feature database based on config passed.\n",
        "    super().__init__(\"okvqa_colab\", config, dataset_type, index, *args, **kwargs)\n",
        "  \n",
        "  def build_annotation_db(self):\n",
        "    \"\"\"\n",
        "    We are overriding this function to return our custom AnnotationDatabase\n",
        "    which we built specifically for OKVQA. Similarly, we can override\n",
        "    build_image_db and build_feature_db for to implement custom versions of\n",
        "    those for a particular dataset.\n",
        "    \"\"\"\n",
        "    annotation_path = self._get_path_based_on_index(\n",
        "        self.config, \"annotations\", self._index\n",
        "    )\n",
        "    return OKVQAAnnotationDatabase(self.config, annotation_path)\n",
        "  \n",
        "  def get_image_path(self, image_id: str):\n",
        "    \"\"\"\n",
        "    Utility function to convert COCO image id to actual image path\n",
        "    \"\"\"\n",
        "    if self.dataset_type == \"train\":\n",
        "      image_path = f\"COCO_train2014_{str(image_id).zfill(12)}.jpg\"\n",
        "    else:\n",
        "      image_path = f\"COCO_val2014_{str(image_id).zfill(12)}.jpg\"\n",
        "    return image_path  \n",
        "\n",
        "  def init_processors(self):\n",
        "    super().init_processors()\n",
        "    # Transforms need to be set to image_processor manually\n",
        "    self.image_db.transform = self.image_processor  \n",
        "\n",
        "  def __getitem__(self, idx: int):\n",
        "    # Take the data sample from the annotation db (basically the annotation)\n",
        "    sample_info = self.annotation_db[idx]\n",
        "    current_sample = Sample()\n",
        "    # Pass it to question processor to process the question\n",
        "    # This in a lot of cases will be defined in the experiment config and\n",
        "    # is interchangable with others. Defaults to glove processor and in\n",
        "    # this colab we will use bert processors defined in the experiment config.\n",
        "    processed_question = self.text_processor({\"text\": sample_info[\"question\"]})\n",
        "    current_sample.update(processed_question)\n",
        "\n",
        "    current_sample.id = torch.tensor(\n",
        "        int(sample_info[\"question_id\"]), dtype=torch.int\n",
        "    )\n",
        "\n",
        "    # Get the first image from the set of images returned from the image_db\n",
        "    image_path = self.get_image_path(sample_info[\"image_id\"])\n",
        "    current_sample.image = self.image_db.from_path(image_path)[\"images\"][0]\n",
        "\n",
        "    if \"answers\" in sample_info:\n",
        "      answers = self.answer_processor({\"answers\": sample_info[\"answers\"]})\n",
        "      current_sample.targets = answers[\"answers_scores\"]\n",
        "    return current_sample\n",
        "  \n",
        "  def visualize(\n",
        "      self, \n",
        "      num_samples: int=1, \n",
        "      use_transforms: bool=False, \n",
        "      *args, \n",
        "      **kwargs\n",
        "  ):\n",
        "    \"\"\"\n",
        "    Visualize function displays `num_samples` number of samples from the dataset.\n",
        "    In this case, it will print out the question and a grid of the random images\n",
        "    from the dataset.\n",
        "    \"\"\"\n",
        "    image_paths = []\n",
        "    random_samples = np.random.randint(0, len(self), size=num_samples)\n",
        "    questions = []\n",
        "    for idx in random_samples:\n",
        "        image_paths.append(\n",
        "            self.get_image_path(self.annotation_db[idx][\"image_id\"])\n",
        "        )\n",
        "        questions.append(self.annotation_db[idx][\"question\"])\n",
        "    images = self.image_db.from_path(image_paths, use_transforms=use_transforms)\n",
        "    print(\"\\n\".join(questions))\n",
        "    visualize_images(images[\"images\"], *args, **kwargs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11KXlPXRG9k8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from mmf.common.registry import registry\n",
        "from mmf.datasets.mmf_dataset_builder import MMFDatasetBuilder\n",
        "\n",
        "# Register as okvqa_colab because okvqa already exists in MMF master branch\n",
        "@registry.register_builder(\"okvqa_colab\")\n",
        "class OKVQABuilder(MMFDatasetBuilder):\n",
        "  ZOO_CONFIG_PATH = \"/content/configs/zoo.yaml\"\n",
        "  def __init__(\n",
        "      self, \n",
        "      dataset_name=\"okvqa_colab\", \n",
        "      dataset_class=OKVQADataset, \n",
        "      *args, \n",
        "      **kwargs,\n",
        "  ):\n",
        "    super().__init__(dataset_name, dataset_class, *args, **kwargs)\n",
        "  \n",
        "  @classmethod\n",
        "  def config_path(cls):\n",
        "    return \"/content/configs/okvqa_colab.yaml\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nk5VcB0MTROC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p /content/configs/ \\\n",
        "  && wget -q https://raw.githubusercontent.com/facebookresearch/mmf/master/mmf/configs/datasets/okvqa/defaults.yaml \\\n",
        "  -O \"/content/configs/okvqa_colab.yaml\" \\\n",
        "  && sed -i \"s/okvqa:/okvqa_colab:/g\" /content/configs/okvqa_colab.yaml \\\n",
        "  && sed -i \"s/okvqa\\//okvqa_colab\\//g\" /content/configs/okvqa_colab.yaml \\\n",
        "  && sed -i \"s/okvqa\\.defaults/okvqa_colab\\.defaults/g\" /content/configs/okvqa_colab.yaml \\\n",
        "  && cat /content/configs/okvqa_colab.yaml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnrHBrezQZg1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import yaml\n",
        "\n",
        "\n",
        "zoo_config = {\n",
        "    \"okvqa_colab\": {\n",
        "        \"defaults\": {\n",
        "            \"version\": 1.020200716,\n",
        "            \"resources\": {\n",
        "                \"images\": [\n",
        "                    {\n",
        "                        \"url\": \"mmf://datasets/okvqa/defaults/images/images.tar.gz\",\n",
        "                        \"file_name\": \"images.tar.gz\",\n",
        "                        \"hashcode\": \"674380cee4285a495c398fe9265224052b04454e4599d56b844f0453140ea82f\",\n",
        "                    }\n",
        "                ],\n",
        "                \"extras\": [\n",
        "                    {\n",
        "                        \"url\": \"mmf://datasets/okvqa/defaults/extras.tar.gz\",\n",
        "                        \"file_name\": \"extras.tar.gz\",\n",
        "                        \"hashcode\": \"9a1cc2f2abedcee494917fae853069d8e465dad050307221b7969c9d65ed1b45\",\n",
        "                    }\n",
        "                ],\n",
        "                \"annotations\": [\n",
        "                    {\n",
        "                        \"url\": \"https://okvqa.allenai.org/static/data/mscoco_train2014_annotations.json.zip\",\n",
        "                        \"file_name\": \"mscoco_train2014_annotations.json.zip\",\n",
        "                    },\n",
        "                    {\n",
        "                        \"url\": \"https://okvqa.allenai.org/static/data/mscoco_val2014_annotations.json.zip\",\n",
        "                        \"file_name\": \"mscoco_val2014_annotations.json.zip\",\n",
        "                    },\n",
        "                    {\n",
        "                        \"url\": \"https://okvqa.allenai.org/static/data/OpenEnded_mscoco_train2014_questions.json.zip\",\n",
        "                        \"file_name\": \"OpenEnded_mscoco_train2014_questions.json.zip\",\n",
        "                    },\n",
        "                    {\n",
        "                        \"url\": \"https://okvqa.allenai.org/static/data/OpenEnded_mscoco_val2014_questions.json.zip\",\n",
        "                        \"file_name\": \"OpenEnded_mscoco_val2014_questions.json.zip\",\n",
        "                    },\n",
        "                ],\n",
        "            },\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(\"/content/configs/zoo.yaml\", \"w\") as f:\n",
        "  yaml.dump(zoo_config, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izxPg4-xUe2b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from mmf.utils.build import build_dataset\n",
        "from mmf.utils.configuration import Configuration\n",
        "\n",
        "# Init configuration to register resolvers\n",
        "Configuration()\n",
        "dataset = build_dataset(\"okvqa_colab\", dataset_type=\"val\")\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams[\"figure.figsize\"] = (20, 20)\n",
        "dataset.visualize(num_samples=8, size=(512, 512), nrow=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1ICH5_Mnc9F",
        "colab_type": "text"
      },
      "source": [
        "# **Model**\n",
        "\n",
        "Now we will build a model for the OKVQA task. This requires the model to take multiple inputs(text questions as well as images). MMF has built-in support for multimodal tasks and offers tons of features to easily build multimodal models. In this specific case we will build a trasnformer model for the OKVQA task. \n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKVdhiRRvtCv",
        "colab_type": "text"
      },
      "source": [
        "## Modality Encoders and Multimodal Embedding\n",
        "\n",
        "For the different modalities we need to first have encoders for each and then process these encoded inputs and build embeddings for each. In this section we will define our Image encoder class for the image input. MMF datasets already preprocess the text input and generate tokens by passing the text through a Bert Tokenizer encoder(we will specify which type of  in our configs later). First we will build an embedding class, which will generate a combined embedding for our multimodal inputs. We will generate three types of embeddings for each modality first which takes three types of tokens:\n",
        "\n",
        "1. Input ID tokens (which are either text tokens or image features)\n",
        "2. Position ID tokens (position of the text tokens or image features)\n",
        "3. Segment ID tokens (token type , either text or image to differentiate)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWdF-Rvncm_E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from typing import Any, Dict\n",
        "\n",
        "import torch\n",
        "from torch import Tensor, nn\n",
        "\n",
        "from mmf.modules.encoders import MultiModalEncoderBase\n",
        "\n",
        "\n",
        "class ImageEncoder(MultiModalEncoderBase):\n",
        "    \"\"\"Extends the MultiModalEncoderBase class which builds the encoder based on\n",
        "     the config parameters. We can set the type of image encoder(resnet50, \n",
        "     resnet152, resnext50 etc) and other parameters like num of features, \n",
        "     type of pooling etc.\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "    def build(self):\n",
        "        self.encoder = self._build_modal_encoder(self.config.image_encoder)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.encoder(x)\n",
        "\n",
        "\n",
        "class MMFTransformerEmbeddings(nn.Module):\n",
        "    \"\"\"Embedding class takes two types of modalities(image and text), each can \n",
        "    have their input id, position id and segment id. We generate embeddings of \n",
        "    dimension config.hidden_size for each and then first add the three embeddings\n",
        "    for each modality to have a modality specific embedding. We then concat the \n",
        "    modality specific embeddings to have a joint embedding.\n",
        "    \"\"\"\n",
        "    def __init__(self, config, transformer, img_dim, img_pos_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        # Text Embeddings\n",
        "        self.word_embeddings = transformer.embeddings.word_embeddings\n",
        "        self.position_embeddings = transformer.embeddings.position_embeddings\n",
        "        self.layer_norm = transformer.embeddings.LayerNorm\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "        # Image Embeddings\n",
        "        self.img_embeddings = nn.Sequential(\n",
        "            nn.Linear(img_dim, config.hidden_size),\n",
        "            torch.nn.LayerNorm(config.hidden_size, eps=1e-12),\n",
        "        )\n",
        "        self.img_pos_embeddings = nn.Sequential(\n",
        "            nn.Linear(img_pos_dim, config.hidden_size),\n",
        "        )\n",
        "        self.img_layer_norm = torch.nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
        "        self.img_dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "        # Token Type Embeddings\n",
        "        self.token_type_embeddings = transformer.embeddings.token_type_embeddings\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: Dict[str, Tensor],\n",
        "        position_ids: Dict[str, Tensor],\n",
        "        segment_ids: Dict[str, Tensor],\n",
        "    ):\n",
        "        ## Calculate text embeddings for word, position, segment type\n",
        "        words_embeddings = self.word_embeddings(input_ids[\"text\"])\n",
        "        # Add position ids for text tokens\n",
        "        if \"text\" not in position_ids:\n",
        "            position_ids[\"text\"] = input_ids[\"text\"].new_tensor(\n",
        "                torch.arange(0, input_ids[\"text\"].size(1), dtype=torch.long)\n",
        "                .unsqueeze(0)\n",
        "                .expand(input_ids[\"text\"].size(0), input_ids[\"text\"].size(1))\n",
        "            )\n",
        "        position_embeddings = self.position_embeddings(position_ids[\"text\"])\n",
        "        if \"text\" not in segment_ids:\n",
        "            segment_ids[\"text\"] = torch.zeros_like(input_ids[\"text\"])\n",
        "        txt_type_embeddings = self.token_type_embeddings(segment_ids[\"text\"])\n",
        "\n",
        "        txt_embeddings = self.layer_norm(\n",
        "            words_embeddings + position_embeddings + txt_type_embeddings\n",
        "        )\n",
        "        txt_embeddings = self.dropout(txt_embeddings)\n",
        "\n",
        "        ## Calculate image embeddings for feature, position, segment type\n",
        "        transformed_input = self.img_embeddings(input_ids[\"image\"])\n",
        "        img_embeddings = transformed_input\n",
        "        if \"image\" in position_ids:\n",
        "            transformed_pos = self.position_embeddings(position_ids[\"image\"])\n",
        "            img_embeddings += transformed_pos\n",
        "\n",
        "        if \"image\" not in segment_ids:\n",
        "            segment_ids[\"image\"] = torch.zeros_like(\n",
        "                input_ids[\"image\"][:, :, 0], dtype=torch.long\n",
        "            )\n",
        "        img_type_embeddings = self.token_type_embeddings(segment_ids[\"image\"])\n",
        "        img_embeddings += img_type_embeddings\n",
        "\n",
        "        img_embeddings = self.img_dropout(self.img_layer_norm(img_embeddings))\n",
        "\n",
        "        return torch.cat([txt_embeddings, img_embeddings], dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcQ-i2SRcFIH",
        "colab_type": "text"
      },
      "source": [
        "## Transformer based multimodal model\n",
        "\n",
        "Here we will start to implement our model. MMF comes out of the box with the required boilerplate structure to build transformer models for solving multimodal tasks. MMF has a `BaseTransformer` class which comes with some building blocks to easily import transformer models from [Huggingface transformers](https://github.com/huggingface/transformers) library and modify them for multi-modal use cases. Extending this base class, we will just need to implement few methods which are specific for our model. These are:\n",
        "\n",
        "```\n",
        "  self.build_transformer()\n",
        "  self.build_encoders()\n",
        "  self.build_embeddings()\n",
        "  self.build_heads()\n",
        "  self.build_losses()\n",
        "```\n",
        "\n",
        "The methods are self explanatory and we will definee each in detail as we build them. `build_transformer()` method is optional to override as base class provides ability load any transformer model from [Huggingface transformers](https://github.com/huggingface/transformers) just by specifying the name of the model. For example in your model config you can specify \n",
        "```\n",
        "\"transformer_base\": \"bert-base-uncased\"\n",
        "```\n",
        "and the BERT Base model with pretrained weights will be available inside the class as `self.transformer`. We will need the encoder and embeddings from this transformer model, which can be accessed by `self.transformer.encoder` and `self.transformer.embeddings`. \n",
        "\n",
        "Oncce we initialize the different components of our model, we will implement the forward pass. The flow that we will use to implement it is as follows:\n",
        "\n",
        "```\n",
        "\n",
        "                    preprocess_sample                          ||\n",
        "                            |                                  ||\n",
        "                   generate embeddings                         ||\n",
        "                            |                                  ||\n",
        "                 generate attention masks                      ||     MODEL\n",
        "                            |                                  ||\n",
        "                 transformer encoder pass                      ||     FLOW\n",
        "                            |                                  ||\n",
        "                   different head pass                         ||   DIRECTION\n",
        "                            |                                  ||\n",
        "                   postprocess_output                          ||\n",
        "                            |                                  ||\n",
        "                 Dict[str, Tensor] output                      \\/\n",
        "\n",
        "        Returns:\n",
        "            Dict[str, Tensor]: Dict containing scores or losses\n",
        "```\n",
        "\n",
        "The input to a models' forward pass in MMF is a `SampleList`. We will preprocess the `SampleList` to generate the required inputs to pass to the embedding layer.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUqfmU0iy8N9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers.modeling_bert import BertPooler, BertPredictionHeadTransform\n",
        "\n",
        "from mmf.common.registry import registry\n",
        "from mmf.common.typings import DictConfig\n",
        "from mmf.models.transformers.base import BaseTransformer, BaseTransformerInput, BaseTransformerConfigType\n",
        "\n",
        "@registry.register_model(\"mmf_transformer\")\n",
        "class MMFTransformer(BaseTransformer):\n",
        "    def __init__(self, config: BaseTransformerConfigType):\n",
        "        super().__init__(config)\n",
        "\n",
        "    @classmethod\n",
        "    def config_path(cls):\n",
        "        return \"/content/mmf_transformer_config.yaml\"\n",
        "\n",
        "    def build_encoders(self):\n",
        "        self.image_encoder = ImageEncoder(self.config)\n",
        "\n",
        "    def build_embeddings(self):\n",
        "        # Here we initialize the embedding class we will use for multiple \n",
        "        # modalities (here just text and image). For the text embeeddings we \n",
        "        # will use the pretrained weights from the trasnformer model rather than\n",
        "        # training from scratch.\n",
        "        self.embeddings = MMFTransformerEmbeddings(\n",
        "            self.transformer_config,\n",
        "            self.transformer,\n",
        "            self.config.visual_embedding_dim,\n",
        "            self.config.visual_position_dim,\n",
        "        )\n",
        "\n",
        "    def build_heads(self):\n",
        "        # Here we initialize the classifier head. It takes the output of the \n",
        "        # transformer encoder and passes it through a pooler (we use the pooler \n",
        "        # from BERT model), then dropout, BertPredictionHeadTransform (which is \n",
        "        # a liner layer, followed by activation and layer norm) and lastly a \n",
        "        # linear layer projecting the hidden output to classification \n",
        "        # labels.\n",
        "        self.classifier = nn.Sequential(\n",
        "            BertPooler(self.transformer_config),\n",
        "            nn.Dropout(self.transformer_config.hidden_dropout_prob),\n",
        "            BertPredictionHeadTransform(self.transformer_config),\n",
        "            nn.Linear(self.transformer_config.hidden_size, self.config.num_labels),\n",
        "        )\n",
        "\n",
        "    def preprocess_sample(self, sample_list: Dict[str, Any]) -> BaseTransformerInput:\n",
        "        # Here we preprocess the sample list elements and form a BaseTransformerInput\n",
        "        # type object. This object standardizes how we represent multiple modalities.\n",
        "        # Check the definition of this dataclass in BaseTransformer.\n",
        "\n",
        "        # Input IDs (or text tokens/image features)\n",
        "        input_ids: Dict[str, Tensor] = {}\n",
        "        input_ids[\"text\"] = sample_list.input_ids\n",
        "        if \"image_feature_0\" in sample_list:\n",
        "            input_ids[\"image\"] = sample_list.image_feature_0\n",
        "        elif \"image\" in sample_list:\n",
        "            input_ids[\"image\"] = self.image_encoder(sample_list.image)\n",
        "\n",
        "        # Position IDs\n",
        "        position_ids: Dict[str, Tensor] = {}\n",
        "        position_ids[\"image\"] = input_ids[\"image\"].new_tensor(\n",
        "            torch.arange(0, input_ids[\"image\"].size(1), dtype=torch.long)\n",
        "            .unsqueeze(0)\n",
        "            .expand(input_ids[\"image\"].size(0), input_ids[\"image\"].size(1)),\n",
        "            dtype=torch.long,\n",
        "        )\n",
        "\n",
        "        # Segment IDs\n",
        "        segment_ids: Dict[str, Tensor] = {}\n",
        "        segment_ids[\"text\"] = sample_list.segment_ids\n",
        "\n",
        "        # Masks\n",
        "        masks: Dict[str, Tensor] = {}\n",
        "        masks[\"text\"] = sample_list.input_mask\n",
        "        if \"image_mask\" in sample_list:\n",
        "            masks[\"image\"] = sample_list.image_mask\n",
        "        else:\n",
        "            masks[\"image\"] = torch.ones_like(\n",
        "                input_ids[\"image\"][:, :, 0], dtype=torch.long\n",
        "            )\n",
        "\n",
        "        return BaseTransformerInput(input_ids, position_ids, segment_ids, masks)\n",
        "\n",
        "    def forward(self, sample_list: Dict[str, Any]) -> Dict[str, Tensor]:\n",
        "        # Sample preprocess\n",
        "        output = self.preprocess_sample(sample_list)\n",
        "\n",
        "        # Transformer Input Embeddings\n",
        "        embedding_output = self.embeddings(\n",
        "            input_ids=output.input_ids,\n",
        "            position_ids=output.position_ids,\n",
        "            segment_ids=output.segment_ids,\n",
        "        )\n",
        "\n",
        "        # Transformer Attention mask\n",
        "        # concat the attention masks for text and image\n",
        "        attention_mask = torch.cat(\n",
        "            (output.masks[\"text\"], output.masks[\"image\"]), dim=-1\n",
        "        )\n",
        "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
        "\n",
        "        # Transformer Encoder\n",
        "        encoded_layers = self.transformer.encoder(\n",
        "            embedding_output,  # combined embedding\n",
        "            extended_attention_mask,  # combined attention mask\n",
        "            [None] * len(self.transformer.encoder.layer),  # head masks\n",
        "        )\n",
        "\n",
        "        # Transformer Heads\n",
        "        head_output = self.classifier(encoded_layers[0])\n",
        "\n",
        "        # Postprocess outputs\n",
        "        return self.postprocess_output(head_output)\n",
        "\n",
        "    def postprocess_output(self, output: Tensor) -> Dict[str, Tensor]:\n",
        "        # Here we postprocess the output from the classifier head and reshape it.\n",
        "        # This will be used to calculate losses and metrics in mmf.\n",
        "        output_dict = {}\n",
        "        output_dict[\"scores\"] = output.contiguous().view(-1, self.config.num_labels)\n",
        "        return output_dict\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F54E2dpSmdKg",
        "colab_type": "text"
      },
      "source": [
        "## Model Configuration\n",
        "\n",
        "Here we build the configuration for the model we just implemented. Some of the config parameters are required to initialize weights for different layers of the model. We also define the image encoder parameters here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSYIqp6MzIzC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a model configuration file\n",
        "mmf_transformer_config = {\n",
        "   \"model_config\":{\n",
        "      \"mmf_transformer\":{\n",
        "         \"transformer_base\": \"bert-base-uncased\",\n",
        "         \"training_head_type\": \"classification\",\n",
        "         \"visual_embedding_dim\": 2048,\n",
        "         \"visual_position_dim\": 1,\n",
        "         \"type_vocab_size\": 2,\n",
        "         \"modalities\":[\n",
        "            \"text\",\n",
        "            \"image\"\n",
        "         ],\n",
        "         \"initializer_range\": 0.02,\n",
        "         \"initializer_mean\": 0.0,\n",
        "         \"layer_norm_weight_fill\": 1.0,\n",
        "         \"random_initialize\": False,\n",
        "         \"freeze_base\": False,\n",
        "         \"finetune_lr_multiplier\": 1,\n",
        "         \"image_encoder\": {\n",
        "            \"type\": \"resnet152\",\n",
        "            \"params\": {\n",
        "               \"pretrained\": True,\n",
        "               \"pool_type\": \"avg\",\n",
        "               \"num_output_features\": 1\n",
        "            }\n",
        "         }\n",
        "      }\n",
        "   }\n",
        "}\n",
        "\n",
        "# Save the dict into a yaml config file to load with mmf\n",
        "import yaml\n",
        "with open(\"/content/mmf_transformer_config.yaml\", \"w+\") as f:\n",
        "  yaml.dump(mmf_transformer_config, f, allow_unicode=True, default_flow_style=False)\n",
        "\n",
        "# Display contents of the saved yaml file\n",
        "!cat /content/mmf_transformer_config.yaml\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DVoDI87XYny",
        "colab_type": "text"
      },
      "source": [
        "## Experiment Configuration\n",
        "\n",
        "Next we will create the configuration for our experiment. Here we will add configurations for components like scheduler, optimizer, evaluation metrics, training metrics and also override some default dataset configurations specifically for our model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8T3DoDw9EQ6r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create an experiment configuration file\n",
        "experiment_config = {\n",
        "   # Scheduler to be used, type `warmup_linear`\n",
        "   \"scheduler\":{\n",
        "      \"type\":\"warmup_linear\",\n",
        "      \"params\":{\n",
        "         \"num_warmup_steps\":2000,\n",
        "         \"num_training_steps\":22000,\n",
        "      }\n",
        "   },\n",
        "   # Override the text processor for the dataset. Since we are using a \n",
        "   # transformer based model with a BERT base, we will override the tokenizer \n",
        "   # with a BERT tokenizer.\n",
        "   \"dataset_config\":{\n",
        "      \"okvqa_colab\":{\n",
        "         \"processors\":{\n",
        "            \"text_processor\":{\n",
        "               \"type\":\"bert_tokenizer\",\n",
        "               \"params\":{\n",
        "                  \"tokenizer_config\":{\n",
        "                     \"type\":\"bert-base-uncased\",\n",
        "                     \"params\":{\n",
        "                        \"do_lower_case\":True\n",
        "                     }\n",
        "                  },\n",
        "                  \"mask_probability\":0,\n",
        "                  \"max_seq_length\":128\n",
        "               }\n",
        "            }\n",
        "         }\n",
        "      }\n",
        "   },\n",
        "   # Some model specific configurations. We define which type of training head \n",
        "   # we want to use (this is useful when multiple different training heads are \n",
        "   # defined for the model). We also specify the number of classification labels\n",
        "   # and type of loss to be used.\n",
        "   \"model_config\":{\n",
        "      \"mmf_transformer\":{\n",
        "         \"training_head_type\":\"classification\",\n",
        "         \"num_labels\":2253,\n",
        "         \"losses\":[\n",
        "            {\n",
        "               \"type\":\"logit_bce\"\n",
        "            }\n",
        "         ]\n",
        "      }\n",
        "   },\n",
        "   # Optimizer used is AdamW with LR 5e-05\n",
        "   \"optimizer\":{\n",
        "      \"type\":\"adam_w\",\n",
        "      \"params\":{\n",
        "         \"lr\":5e-05,\n",
        "         \"eps\":1e-08\n",
        "      }\n",
        "   },\n",
        "   # OKVQA uses same metric as VQA evaluation\n",
        "   \"evaluation\":{\n",
        "      \"metrics\":[\n",
        "         \"vqa_accuracy\"\n",
        "      ]\n",
        "   },\n",
        "   # We change some training parameters here. \n",
        "   \"training\":{\n",
        "      \"batch_size\":32,\n",
        "      \"lr_scheduler\": True,\n",
        "      \"max_updates\":22000,\n",
        "      \"early_stop\":{\n",
        "         \"criteria\":\"okvqa_colab/vqa_accuracy\",\n",
        "         \"minimize\": False\n",
        "      }\n",
        "   },\n",
        "}\n",
        "\n",
        "# Save the dict into a yaml config file to load with mmf\n",
        "import yaml\n",
        "with open(\"/content/experiment_config.yaml\", \"w+\") as f:\n",
        "  yaml.dump(experiment_config, f, allow_unicode=True, default_flow_style=False)\n",
        "\n",
        "# Display and verify contents of the saved yaml file\n",
        "!cat /content/experiment_config.yaml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWPGXlXJbAGI",
        "colab_type": "text"
      },
      "source": [
        "# Running the experiment\n",
        "\n",
        "We now have all the components ready to start our experiment. MMF provides cli commands for running training, prediction etc and those can also be executed as function calls. Here we show how to  and send them as params to the `run` method.\n",
        "\n",
        "We will also unregister processor register to registry during visualization of dataset to avoid any caches of other datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3U-FlP3JWOZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from mmf.common.registry import registry\n",
        "\n",
        "_ = registry.unregister(\"okvqa_colab_text_processor\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GPAKFMUHYht",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from mmf_cli.run import run\n",
        "\n",
        "opts = [\n",
        "        \"config=/content/experiment_config.yaml\",\n",
        "        \"model=mmf_transformer\",\n",
        "        \"dataset=okvqa_colab\",\n",
        "        \"training.log_interval=10\",\n",
        "        \"training.batch_size=8\",\n",
        "        \"training.max_updates=100\",\n",
        "        # Head is in dangling state, ignore git details in checkpoints\n",
        "        \"checkpoint.save_git_details=False\",\n",
        "]\n",
        "\n",
        "\n",
        "run(opts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXfI1zSXAYML",
        "colab_type": "text"
      },
      "source": [
        "# Inference on validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0JBPERks8pI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Download the pretrained model\n",
        "!wget https://dl.fbaipublicfiles.com/mmf/data/okvqa_mmft.ckpt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrRJF_DcIqRh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model inference on validation set\n",
        "from mmf_cli.run import run\n",
        "\n",
        "\n",
        "opts = [\n",
        "        \"config=/content/experiment_config.yaml\",\n",
        "        \"model=mmf_transformer\",\n",
        "        \"dataset=okvqa_colab\",\n",
        "        \"run_type=val\",\n",
        "        \"training.batch_size=8\",\n",
        "        \"checkpoint.resume_file=okvqa_mmft.ckpt\",\n",
        "        # Head is in dangling state, ignore git details in checkpoints\n",
        "        \"checkpoint.save_git_details=False\",\n",
        "]\n",
        "\n",
        "\n",
        "run(opts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPpyPKKFAk-h",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation on test samples\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfgWPcaaAuSG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build config, processors and model\n",
        "\n",
        "## Build configuration\n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "dataset_conf = OmegaConf.load('/content/configs/okvqa_colab.yaml')\n",
        "model_conf = OmegaConf.load('/content/mmf_transformer_config.yaml')\n",
        "experiment_conf = OmegaConf.load('/content/experiment_config.yaml')\n",
        "extra_args = [\"env.data_dir=/root/.cache/torch/mmf/data/\"]\n",
        "extra_args = OmegaConf.from_dotlist(extra_args)\n",
        "\n",
        "conf = OmegaConf.merge(dataset_conf, model_conf, experiment_conf, extra_args)\n",
        "\n",
        "conf.dataset_config.okvqa_colab.processors.answer_processor.params.vocab_file=\"/root/.cache/torch/mmf/data/datasets/\" + conf.dataset_config.okvqa_colab.processors.answer_processor.params.vocab_file\n",
        "\n",
        "## Build processors\n",
        "\n",
        "from mmf.utils.build import build_processors\n",
        "\n",
        "mmf_processors = build_processors(conf.dataset_config.okvqa_colab.processors)\n",
        "\n",
        "\n",
        "## Build model\n",
        "model = MMFTransformer(conf.model_config.mmf_transformer)\n",
        "model.build()\n",
        "model.init_losses()\n",
        "\n",
        "state_dict = torch.load('okvqa_mmft.ckpt')\n",
        "model.load_state_dict(state_dict[\"model\"])\n",
        "model.to(\"cuda\")\n",
        "model.eval()\n",
        "print(\"Model Loaded Successfully!!\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHcb7S_Eopyj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import urllib.request\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.datasets.folder as tv_helpers\n",
        "\n",
        "from mmf.common.sample import Sample, SampleList\n",
        "\n",
        "\n",
        "def create_sample(image, text):\n",
        "  # Create a Sample\n",
        "  current_sample = Sample()\n",
        "\n",
        "  # Preprocess the text to generate tokens\n",
        "  processed_text = mmf_processors[\"text_processor\"]({\"text\": text})\n",
        "  current_sample.update(processed_text)\n",
        "  \n",
        "  # Load the image and run image preprocessors on it\n",
        "  current_sample.image = mmf_processors[\"image_processor\"](image)\n",
        "\n",
        "  # Create a sample list\n",
        "  sample_list = SampleList([current_sample])\n",
        "  sample_list = sample_list.to(\"cuda\")\n",
        "  return sample_list\n",
        "\n",
        "image_url = \"http://images.cocodataset.org/train2017/000000444444.jpg\" #@param {type:\"string\"}\n",
        "question = \"Which sport requires riding on the animal depicted?\" #@param {type:\"string\"}\n",
        "urllib.request.urlretrieve(image_url, \"/content/local.jpg\")\n",
        "image = tv_helpers.default_loader(\"/content/local.jpg\")\n",
        "print(\"Image :: \\n\")\n",
        "plt.imshow(image)\n",
        "print(\"Question :: \", question)\n",
        "\n",
        "output = model(create_sample(image, question))\n",
        "output = torch.nn.functional.softmax(output[\"scores\"], dim=1)\n",
        "prob, indices = output.topk(1, dim=1)\n",
        "answer = mmf_processors[\"answer_processor\"].idx2word(indices[0][0])\n",
        "print(answer)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
