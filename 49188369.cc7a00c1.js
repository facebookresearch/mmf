(window.webpackJsonp=window.webpackJsonp||[]).push([[14],{150:function(e,t,a){"use strict";a.r(t),a.d(t,"frontMatter",(function(){return o})),a.d(t,"metadata",(function(){return s})),a.d(t,"rightToc",(function(){return b})),a.d(t,"default",(function(){return l}));var r=a(2),n=a(9),i=(a(0),a(168)),o={id:"dataset_zoo",title:"Dataset Zoo",sidebar_label:"Dataset Zoo"},s={id:"notes/dataset_zoo",title:"Dataset Zoo",description:"Here is the current list of datasets currently supported in MMF:",source:"@site/docs/notes/dataset_zoo.md",permalink:"/docs/notes/dataset_zoo",editUrl:"https://github.com/facebookresearch/mmf/edit/master/website/docs/notes/dataset_zoo.md",lastUpdatedBy:"Amanpreet Singh",lastUpdatedAt:1593555534,sidebar_label:"Dataset Zoo",sidebar:"docs",previous:{title:"MMF's Configuration System",permalink:"/docs/notes/configuration"},next:{title:"Model Zoo",permalink:"/docs/notes/model_zoo"}},b=[],p={rightToc:b};function l(e){var t=e.components,a=Object(n.a)(e,["components"]);return Object(i.b)("wrapper",Object(r.a)({},p,a,{components:t,mdxType:"MDXLayout"}),Object(i.b)("p",null,"Here is the current list of datasets currently supported in MMF:"),Object(i.b)("ul",null,Object(i.b)("li",{parentName:"ul"},Object(i.b)("strong",{parentName:"li"},"CLEVR")," ","[Paper : CLEVR A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning][arxiv]","(",Object(i.b)("a",Object(r.a)({parentName:"li"},{href:"https://arxiv.org/abs/1612.06890)%5D"}),"https://arxiv.org/abs/1612.06890)]")),Object(i.b)("li",{parentName:"ul"},Object(i.b)("strong",{parentName:"li"},"COCO Captions")," ","[Paper : Microsoft COCO Captions: Data Collection and Evaluation Server][arxiv]","(",Object(i.b)("a",Object(r.a)({parentName:"li"},{href:"https://arxiv.org/abs/1504.00325)%5D"}),"https://arxiv.org/abs/1504.00325)]")),Object(i.b)("li",{parentName:"ul"},Object(i.b)("strong",{parentName:"li"},"Conceptual Captions")," ","[Paper : Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning][website]","(",Object(i.b)("a",Object(r.a)({parentName:"li"},{href:"https://ai.google.com/research/ConceptualCaptions)%5D"}),"https://ai.google.com/research/ConceptualCaptions)]")),Object(i.b)("li",{parentName:"ul"},Object(i.b)("strong",{parentName:"li"},"Hateful Memes")," ","[Paper : The Hateful Memes Challenge: Detecting Hate Speech in Multimodal Memes][arxiv]","(",Object(i.b)("a",Object(r.a)({parentName:"li"},{href:"https://arxiv.org/abs/2005.04790)%5D"}),"https://arxiv.org/abs/2005.04790)]")),Object(i.b)("li",{parentName:"ul"},Object(i.b)("strong",{parentName:"li"},"MM IMDB")," ","[Paper : Gated Multimodal Units for Information Fusion][arxiv]","(",Object(i.b)("a",Object(r.a)({parentName:"li"},{href:"https://arxiv.org/abs/1702.01992)%5D"}),"https://arxiv.org/abs/1702.01992)]")," [",Object(i.b)("a",Object(r.a)({parentName:"li"},{href:"http://lisi1.unal.edu.co/mmimdb"}),"website"),"]"),Object(i.b)("li",{parentName:"ul"},Object(i.b)("strong",{parentName:"li"},"NLVR2")," ","[Paper : A Corpus for Reasoning About Natural Language Grounded in Photographs][arxiv]","(",Object(i.b)("a",Object(r.a)({parentName:"li"},{href:"https://arxiv.org/abs/1811.00491)%5D"}),"https://arxiv.org/abs/1811.00491)]")," [",Object(i.b)("a",Object(r.a)({parentName:"li"},{href:"http://lil.nlp.cornell.edu/nlvr/"}),"website"),"]"),Object(i.b)("li",{parentName:"ul"},Object(i.b)("strong",{parentName:"li"},"OCRVQA")," ","[Paper : OCR-VQA: Visual Question Answering by Reading Text in Images][website]","(",Object(i.b)("a",Object(r.a)({parentName:"li"},{href:"https://ocr-vqa.github.io/)%5D"}),"https://ocr-vqa.github.io/)]")),Object(i.b)("li",{parentName:"ul"},Object(i.b)("strong",{parentName:"li"},"STVQA")," ","[Paper : Scene Text Visual Question Answering][arxiv]","(",Object(i.b)("a",Object(r.a)({parentName:"li"},{href:"https://arxiv.org/abs/1905.13648)%5D"}),"https://arxiv.org/abs/1905.13648)]")),Object(i.b)("li",{parentName:"ul"},Object(i.b)("strong",{parentName:"li"},"TextVQA")," ","[Paper : Towards VQA Models That Can Read][arxiv]","(",Object(i.b)("a",Object(r.a)({parentName:"li"},{href:"https://arxiv.org/abs/1904.08920)%5D"}),"https://arxiv.org/abs/1904.08920)]")," [",Object(i.b)("a",Object(r.a)({parentName:"li"},{href:"https://textvqa.org/"}),"website"),"]"),Object(i.b)("li",{parentName:"ul"},Object(i.b)("strong",{parentName:"li"},"TextCaps")," ","[Paper : TextCaps: a Dataset for Image Captioning with Reading Comprehension][arxiv]","(",Object(i.b)("a",Object(r.a)({parentName:"li"},{href:"https://arxiv.org/abs/2003.12462)%5D"}),"https://arxiv.org/abs/2003.12462)]")),Object(i.b)("li",{parentName:"ul"},Object(i.b)("strong",{parentName:"li"},"Visual Dialog")," ","[Paper : Visual Dialog][website]","(",Object(i.b)("a",Object(r.a)({parentName:"li"},{href:"https://visualdialog.org/)%5D"}),"https://visualdialog.org/)]")),Object(i.b)("li",{parentName:"ul"},Object(i.b)("strong",{parentName:"li"},"SNLI-VE")," ","[Paper : Visual Entailment: A Novel Task for Fine-Grained Image Understanding][arxiv]","(",Object(i.b)("a",Object(r.a)({parentName:"li"},{href:"https://arxiv.org/abs/1901.06706)%5D"}),"https://arxiv.org/abs/1901.06706)]")," [",Object(i.b)("a",Object(r.a)({parentName:"li"},{href:"https://github.com/necla-ml/SNLI-VE"}),"website"),"]"),Object(i.b)("li",{parentName:"ul"},Object(i.b)("strong",{parentName:"li"},"Visual Genome")," ","[Paper : Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations][arxiv]","(",Object(i.b)("a",Object(r.a)({parentName:"li"},{href:"https://arxiv.org/abs/1602.07332)%5D"}),"https://arxiv.org/abs/1602.07332)]")," [",Object(i.b)("a",Object(r.a)({parentName:"li"},{href:"https://visualgenome.org/"}),"website"),"]"),Object(i.b)("li",{parentName:"ul"},Object(i.b)("strong",{parentName:"li"},"VizWiZ")," ","[Paper : VizWiz Grand Challenge: Answering Visual Questions from Blind People][arxiv]","(",Object(i.b)("a",Object(r.a)({parentName:"li"},{href:"https://arxiv.org/abs/1802.08218)%5D"}),"https://arxiv.org/abs/1802.08218)]")," [",Object(i.b)("a",Object(r.a)({parentName:"li"},{href:"https://vizwiz.org/"}),"website"),"]"),Object(i.b)("li",{parentName:"ul"},Object(i.b)("strong",{parentName:"li"},"VQA2.0")," ","[Paper : VQA: Visual Question Answering][arxiv]","(",Object(i.b)("a",Object(r.a)({parentName:"li"},{href:"https://arxiv.org/abs/1505.00468)%5D"}),"https://arxiv.org/abs/1505.00468)]")," [",Object(i.b)("a",Object(r.a)({parentName:"li"},{href:"https://visualqa.org/"}),"website"),"]")))}l.isMDXComponent=!0},168:function(e,t,a){"use strict";a.d(t,"a",(function(){return c})),a.d(t,"b",(function(){return g}));var r=a(0),n=a.n(r);function i(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function o(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,r)}return a}function s(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?o(Object(a),!0).forEach((function(t){i(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function b(e,t){if(null==e)return{};var a,r,n=function(e,t){if(null==e)return{};var a,r,n={},i=Object.keys(e);for(r=0;r<i.length;r++)a=i[r],t.indexOf(a)>=0||(n[a]=e[a]);return n}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)a=i[r],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(n[a]=e[a])}return n}var p=n.a.createContext({}),l=function(e){var t=n.a.useContext(p),a=t;return e&&(a="function"==typeof e?e(t):s(s({},t),e)),a},c=function(e){var t=l(e.components);return n.a.createElement(p.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return n.a.createElement(n.a.Fragment,{},t)}},m=n.a.forwardRef((function(e,t){var a=e.components,r=e.mdxType,i=e.originalType,o=e.parentName,p=b(e,["components","mdxType","originalType","parentName"]),c=l(a),m=r,g=c["".concat(o,".").concat(m)]||c[m]||u[m]||i;return a?n.a.createElement(g,s(s({ref:t},p),{},{components:a})):n.a.createElement(g,s({ref:t},p))}));function g(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=a.length,o=new Array(i);o[0]=m;var s={};for(var b in t)hasOwnProperty.call(t,b)&&(s[b]=t[b]);s.originalType=e,s.mdxType="string"==typeof e?e:r,o[1]=s;for(var p=2;p<i;p++)o[p]=a[p];return n.a.createElement.apply(null,o)}return n.a.createElement.apply(null,a)}m.displayName="MDXCreateElement"}}]);