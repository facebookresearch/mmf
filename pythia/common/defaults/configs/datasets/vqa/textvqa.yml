dataset_attributes:
  textvqa:
      data_root_dir: ../data
      image_depth_first: false
      fast_read: false
      image_features:
          train:
          - open_images/detectron_fix_100/fc6/train,open_images/resnet152/train
          val:
          - open_images/detectron_fix_100/fc6/train,open_images/resnet152/train
          test:
          - open_images/detectron_fix_100/fc6/test,open_images/resnet152/test
      imdb_files:
          train:
          - imdb/textvqa_0.5/imdb_textvqa_train.npy
          val:
          - imdb/textvqa_0.5/imdb_textvqa_val.npy
          test:
          - imdb/textvqa_0.5/imdb_textvqa_test.npy
      features_max_len: 137
      processors:
        text_processor:
          type: vocab
          params:
            max_length: 14
            vocab:
              type: intersected
              embedding_name: glove.6B.300d
              vocab_file: vocabs/vocabulary_100k.txt
            preprocessor:
              type: simple_sentence
              params: {}
        answer_processor:
          type: vqa_answer
          params:
            vocab_file: vocabs/answers_textvqa_8k.txt
            preprocessor:
              type: simple_word
              params: {}
            num_answers: 10
        context_processor:
          type: fasttext
          params:
            max_length: 50
            model_file: .vector_cache/wiki.en.bin
        ocr_token_processor:
          type: simple_word
          params: {}
        bbox_processor:
          type: bbox
          params:
            max_length: 50
      return_info: true
      # Return OCR information
      use_ocr: true
      # Return spatial information of OCR tokens if present
      use_ocr_info: false
training_parameters:
    monitored_metric: textvqa/vqa_accuracy
    metric_minimize: false
