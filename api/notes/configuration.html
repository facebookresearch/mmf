


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Configuration System &mdash; MMF 1.0.0rc8 documentation</title>
  

  
  
  
  
    <link rel="canonical" href="/notes/configuration.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../_static/css/customize.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Challenge Participation" href="challenge.html" />
    <link rel="prev" title="Pretrained Models" href="pretrained_models.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://mmf.sh/" aria-label="MMF"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://mmf.sh/get-started">Get Started</a>
          </li>


          <li>
            <a href="https://mmf.sh/tutorials">Tutorials</a>
          </li>

          <li class="active">
            <a href="https://mmf.sh/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <a href="https://github.com/facebookresearch/mmf">Github</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>

  </div>
</div>


<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  1.0.0
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Getting Started</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="quickstart.html">Quickstart </a></li>
<li class="toctree-l1"><a class="reference internal" href="features.html">Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="pretrained_models.html">Pretrained Models</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Configuration System</a></li>
<li class="toctree-l1"><a class="reference internal" href="challenge.html">Challenge Participation</a></li>
<li class="toctree-l1"><a class="reference internal" href="faq.html">FAQs</a></li>
</ul>
<p class="caption"><span class="caption-text">Challenges</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="hateful_memes_challenge.html">Hateful Memes</a></li>
</ul>
<p class="caption"><span class="caption-text">Extending MMF</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/concepts.html">Terminology and Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/dataset.html">Tutorial: Adding a dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/late_fusion.html">Tutorial: Late Fusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/mmf_bert.html">Tutorial: Detect Hate Speech with MMFBERT</a></li>
</ul>
<p class="caption"><span class="caption-text">Library Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../lib/common/registry.html">common.registry</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lib/common/sample.html">common.sample</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lib/models/base_model.html">models.base_model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lib/modules/losses.html">modules.losses</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lib/modules/metrics.html">modules.metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lib/datasets/base_dataset_builder.html">datasets.base_dataset_builder</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lib/datasets/base_dataset.html">datasets.base_dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lib/datasets/processors.html">datasets.processors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lib/utils/text.html">utils.text</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>Configuration System</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../_sources/notes/configuration.md.txt" rel="nofollow"><img src="../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <div class="section" id="configuration-system">
<h1>Configuration System<a class="headerlink" href="#configuration-system" title="Permalink to this headline">¶</a></h1>
<p>MMF relies on <a class="reference external" href="https://omegaconf.readthedocs.io/en/latest/">OmegaConf</a> for its configuration system and adds some sugar on top of it. We have developed MMF as a config-first framework. Most of the parameters/settings in MMF are configurable. MMF defines some default configuration settings for its system including datasets and models. Users can then update these values via their own config or a command line dotlist.</p>
<p><strong>TL;DR</strong></p>
<ul class="simple">
<li><p>MMF uses OmegaConf for its configuration system with some sugar on top.</p></li>
<li><p>MMF defines <a class="reference external" href="#base-defaults-config">base defaults config</a> containing all MMF specific parameters and then each dataset and model define their own configs (example configs: <a class="reference internal" href="../others/model_config.html"><span class="doc">[model]</span></a> <a class="reference internal" href="../others/dataset_config.html"><span class="doc">[dataset]</span></a>).</p></li>
<li><p>The user can define its own config specified by <code class="docutils literal notranslate"><span class="pre">config=&lt;x&gt;</span></code> at command line for each unique experiment or training setup. This has higher priority then base, model and dataset default configs and can override anything in those.</p></li>
<li><p>Finally, user can override (highest priority) the final config generated by merge of all above configs by specifying config parameters as <a class="reference external" href="https://omegaconf.readthedocs.io/en/latest/usage.html#from-a-dot-list">dotlist</a> in their command. This is the <strong>recommended</strong> way of overriding the config parameters in MMF.</p></li>
<li><p>How MMF knows which config to pick for dataset and model? The user needs to specify those in his command as <code class="docutils literal notranslate"><span class="pre">model=x</span></code> and <code class="docutils literal notranslate"><span class="pre">dataset=y</span></code>.</p></li>
<li><p>Some of the MMF config parameters under <code class="docutils literal notranslate"><span class="pre">env</span></code> field can be overridden by environment variable. Have a look at them.</p></li>
</ul>
<div class="section" id="omegaconf">
<h2>OmegaConf<a class="headerlink" href="#omegaconf" title="Permalink to this headline">¶</a></h2>
<p>For understanding and using the MMF configuration system to its full extent having a look at <a class="reference external" href="https://omegaconf.readthedocs.io/en/latest/">OmegaConf docs</a> especially the sections on <a class="reference external" href="https://omegaconf.readthedocs.io/en/latest/usage.html#variable-interpolation">interpolation</a>, <a class="reference external" href="https://omegaconf.readthedocs.io/en/latest/usage.html#access-and-manipulation">access</a> and <a class="reference external" href="https://omegaconf.readthedocs.io/en/latest/usage.html#configuration-flags">configuration flags</a>. MMF’s config currently is by default in struct mode and we plan to make it readonly in future.</p>
</div>
<div class="section" id="hierarchy">
<h2>Hierarchy<a class="headerlink" href="#hierarchy" title="Permalink to this headline">¶</a></h2>
<p>MMF follows set hierarchy rules to determine the final configuration values. Following list shows the building blocks of MMF’s configuration in an increasing order of priority (higher rank will override lower rank).</p>
<ul class="simple">
<li><p><a class="reference external" href="#base-defaults-config">Base Defaults Config</a></p></li>
<li><p>Dataset’s Config (defined in dataset’s <code class="docutils literal notranslate"><span class="pre">config_path</span></code> classmethod)</p></li>
<li><p>Model’s Config (defined in model’s <code class="docutils literal notranslate"><span class="pre">config_path</span></code> classmethod)</p></li>
<li><p>User’s Config (Passed by user as <code class="docutils literal notranslate"><span class="pre">config=x</span></code> in command)</p></li>
<li><p>Command Line DotList (Passed by user as <code class="docutils literal notranslate"><span class="pre">x.y.z=v</span></code> dotlist in command)</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Configs other than base defaults can still add new nodes that are not in base defaults config, so user can add their own config parameters if they need to without changing the base defaults. If a node has same path, nodes in higher priority config will override the lower priority nodes.</p>
</div>
</div>
<div class="section" id="base-defaults">
<h2>Base Defaults<a class="headerlink" href="#base-defaults" title="Permalink to this headline">¶</a></h2>
<p>Full base defaults config can be seen <a class="reference external" href="#base-defaults-config">below</a>. This config is base of MMF’s configuration system and is included in all of the experiments. It sets up nodes for training related configuration and those that need to be filled by other configs which are specified by user. Main configuration parameters that base defaults define:</p>
<ul class="simple">
<li><p>training parameters</p></li>
<li><p>distributed training parameters</p></li>
<li><p>env parameters</p></li>
<li><p>evaluation parameters</p></li>
<li><p>checkpoint parameters</p></li>
<li><p>run_type parameters</p></li>
</ul>
</div>
<div class="section" id="dataset-config">
<h2>Dataset Config<a class="headerlink" href="#dataset-config" title="Permalink to this headline">¶</a></h2>
<p>Each dataset <a class="reference internal" href="../lib/common/registry.html"><span class="doc">registered</span></a> to MMF can define its defaults config by specifying it in classmethod <code class="docutils literal notranslate"><span class="pre">config_path</span></code> (<a class="reference external" href="https://github.com/facebookresearch/mmf/blob/ae1689c0e2f9d8f51f337676495057168751c5ea/mmf/datasets/builders/ocrvqa/builder.py#L15">example</a>). If <code class="docutils literal notranslate"><span class="pre">processors</span></code> key whose value is a dictionary is specified, processors will be initialized by the dataset builder. If dataset builder inherits from MMFDatasetBuilder, it will look for <code class="docutils literal notranslate"><span class="pre">annotations</span></code>, <code class="docutils literal notranslate"><span class="pre">features</span></code> and <code class="docutils literal notranslate"><span class="pre">images</span></code> field as well in the configuration. A sample config for a builder inheriting MMFDatasetBuilder would look like:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">dataset_config</span><span class="p">:</span>
    <span class="nt">dataset_registry_key</span><span class="p">:</span>
        <span class="nt">use_images</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">true</span>
        <span class="nt">use_features</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">true</span>
        <span class="nt">annotations</span><span class="p">:</span>
            <span class="nt">train</span><span class="p">:</span>
            <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">...</span>
            <span class="nt">val</span><span class="p">:</span>
            <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">...</span>
            <span class="nt">test</span><span class="p">:</span>
            <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">...</span>
        <span class="nt">images</span><span class="p">:</span>
            <span class="nt">train</span><span class="p">:</span>
            <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">...</span>
            <span class="nt">val</span><span class="p">:</span>
            <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">...</span>
            <span class="nt">test</span><span class="p">:</span>
            <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">...</span>
        <span class="nt">features</span><span class="p">:</span>
            <span class="nt">train</span><span class="p">:</span>
            <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">...</span>
            <span class="nt">val</span><span class="p">:</span>
            <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">...</span>
            <span class="nt">test</span><span class="p">:</span>
            <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">...</span>
        <span class="nt">processors</span><span class="p">:</span>
            <span class="nt">text_processor</span><span class="p">:</span>
                <span class="nt">type</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">x</span>
                <span class="nt">params</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">...</span>
</pre></div>
</div>
<p>Configs for datasets packages with MMF are present at <a class="reference external" href="https://github.com/facebookresearch/mmf/tree/ae1689c0e2f9d8f51f337676495057168751c5ea/mmf/configs/datasets">mmf/configs/datasets</a>. Each dataset also provides composable configs which can be used to use some different from default but standard variation of the datasets. These can be directly included into user config by using <a class="reference external" href="#includes">includes</a> directive.</p>
<p>User needs to specify the dataset they are using by adding <code class="docutils literal notranslate"><span class="pre">dataset=&lt;dataset_key&gt;</span></code> option to their command.</p>
</div>
<div class="section" id="model-config">
<h2>Model Config<a class="headerlink" href="#model-config" title="Permalink to this headline">¶</a></h2>
<p>Similar to dataset config, each model <a class="reference internal" href="../lib/common/registry.html"><span class="doc">registered</span></a> to MMF can define its config. this is defined by model’s <code class="docutils literal notranslate"><span class="pre">config_path</span></code> classmethod (<a class="reference external" href="https://github.com/facebookresearch/mmf/blob/ae1689c0e2f9d8f51f337676495057168751c5ea/mmf/models/cnn_lstm.py#L40">example</a>). Configs for models live at <a class="reference external" href="https://github.com/facebookresearch/mmf/tree/ae1689c0e2f9d8f51f337676495057168751c5ea/mmf/configs/models">mmf/configs/models</a>. Again, like datasets models also provide some variations which can be used by including configs for those variations in the user config.</p>
<p>User needs to specify the model they want to use by adding <code class="docutils literal notranslate"><span class="pre">model=&lt;model_key&gt;</span></code> option to their command. A sample model config would look like:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">model_config</span><span class="p">:</span>
    <span class="nt">model_key</span><span class="p">:</span>
        <span class="nt">random_module</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">...</span>
</pre></div>
</div>
</div>
<div class="section" id="user-config">
<h2>User Config<a class="headerlink" href="#user-config" title="Permalink to this headline">¶</a></h2>
<p>User can specify their configuration specific to an experiment or training setup by adding <code class="docutils literal notranslate"><span class="pre">config=&lt;config_path&gt;</span></code> argument to their command. User config can specify for e.g. training parameters according to their experiment such as batch size using <code class="docutils literal notranslate"><span class="pre">training.batch_size</span></code>. Most common use case for user config is to specify optimizer, scheduler and training parameters. Other than that user config can also include configs for variations of models and datasets they want to test on. Have a look at an example user config <a class="reference internal" href="../others/user_config.html"><span class="doc">here</span></a>.</p>
</div>
<div class="section" id="command-line-dot-list-override">
<h2>Command Line Dot List Override<a class="headerlink" href="#command-line-dot-list-override" title="Permalink to this headline">¶</a></h2>
<p>Updating the configuration through <a class="reference external" href="https://omegaconf.readthedocs.io/en/latest/usage.html#from-a-dot-list">dot list</a> syntax is very helpful when running multiple versions of an experiment without actually updating a config. For example, to override batch size from command line you can add <code class="docutils literal notranslate"><span class="pre">training.batch_size=x</span></code> at the end of your command. Similarly, for overriding an annotation in the hateful memes dataset, you can do <code class="docutils literal notranslate"><span class="pre">dataset_config.hateful_memes.annotations.train[0]=x</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Command Line Dot List overrides are our recommended way of updating config parameters instead of manually updating them in config for every other change.</p>
</div>
</div>
<div class="section" id="includes">
<h2>Includes<a class="headerlink" href="#includes" title="Permalink to this headline">¶</a></h2>
<p>MMF’s configuration system on top of OmegaConf allows building user configs by including composable configs provided by the datasets and models. You can include it following the syntax</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">includes</span><span class="p">:</span>
<span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">path/to/first/yaml/to/be/included.yaml</span>
<span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">second.yaml</span>
</pre></div>
</div>
<p>The configs will override in the sequence of how they appear in the directive. Finally, the config parameters defined in the current config will override what is present in the includes. So, for e.g.</p>
<p>First file, <code class="docutils literal notranslate"><span class="pre">a.yaml</span></code>:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="c1"># a.yaml</span>
<span class="nt">dataset_config</span><span class="p">:</span>
  <span class="nt">hateful_memes</span><span class="p">:</span>
    <span class="nt">max_features</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">80</span>
    <span class="nt">use_features</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">true</span>
  <span class="nt">vqa2</span><span class="p">:</span>
    <span class="nt">use_features</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">true</span>

<span class="nt">model_config</span><span class="p">:</span>
  <span class="nt">mmbt</span><span class="p">:</span>
    <span class="nt">num_classes</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">4</span>
    <span class="nt">features_dim</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">2048</span>
</pre></div>
</div>
<p>Second file, <code class="docutils literal notranslate"><span class="pre">b.yaml</span></code>:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="c1"># b.yaml</span>
<span class="nt">optimizer</span><span class="p">:</span>
  <span class="nt">type</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">adam</span>

<span class="nt">dataset_config</span><span class="p">:</span>
  <span class="nt">hateful_memes</span><span class="p">:</span>
    <span class="nt">max_features</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">90</span>
    <span class="nt">use_features</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">false</span>
    <span class="nt">use_images</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">true</span>
  <span class="nt">vqa2</span><span class="p">:</span>
    <span class="nt">depth_first</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">false</span>
</pre></div>
</div>
<p>And final user config, <code class="docutils literal notranslate"><span class="pre">user.yaml</span></code>:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="c1"># user.yaml</span>
<span class="nt">includes</span><span class="p">:</span>
<span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">a.yaml</span>
<span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">b.yaml</span>

<span class="nt">dataset_config</span><span class="p">:</span>
  <span class="nt">hateful_memes</span><span class="p">:</span>
    <span class="nt">max_features</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">100</span>
  <span class="nt">vqa2</span><span class="p">:</span>
    <span class="nt">annotations</span><span class="p">:</span>
      <span class="nt">train</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">x.npy</span>

<span class="nt">model_config</span><span class="p">:</span>
  <span class="nt">mmbt</span><span class="p">:</span>
    <span class="nt">num_classes</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">2</span>
</pre></div>
</div>
<p>would result in final config:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">dataset_config</span><span class="p">:</span>
  <span class="nt">hateful_memes</span><span class="p">:</span>
    <span class="nt">max_features</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">100</span>
    <span class="nt">use_features</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">false</span>
    <span class="nt">use_images</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">true</span>
  <span class="nt">vqa2</span><span class="p">:</span>
    <span class="nt">use_features</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">true</span>
    <span class="nt">depth_first</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">false</span>
    <span class="nt">annotations</span><span class="p">:</span>
      <span class="nt">train</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">x.npy</span>

<span class="nt">model_config</span><span class="p">:</span>
  <span class="nt">mmbt</span><span class="p">:</span>
    <span class="nt">num_classes</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">2</span>
    <span class="nt">features_dim</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">2048</span>

<span class="nt">optimizer</span><span class="p">:</span>
  <span class="nt">type</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">adam</span>
</pre></div>
</div>
</div>
<div class="section" id="other-overrides">
<h2>Other overrides<a class="headerlink" href="#other-overrides" title="Permalink to this headline">¶</a></h2>
<p>We also support some useful overrides schemes at the same level of command line dot list override. For example, user can specify their overrides in form of <a class="reference external" href="https://pypi.org/project/demjson/">demjson</a> as value to argument <code class="docutils literal notranslate"><span class="pre">--config_override</span></code> which will them override each part of config accordingly.</p>
</div>
<div class="section" id="environment-variables">
<h2>Environment Variables<a class="headerlink" href="#environment-variables" title="Permalink to this headline">¶</a></h2>
<p>MMF supports overriding some of the config parameters through environment variables. Have a look at them in <a class="reference external" href="#base-defaults-config">base default config</a>’s <code class="docutils literal notranslate"><span class="pre">env</span></code> parameters.</p>
</div>
<div class="section" id="base-defaults-config">
<h2>Base Defaults Config<a class="headerlink" href="#base-defaults-config" title="Permalink to this headline">¶</a></h2>
<p>Have a look at the defaults config of MMF along with description of parameters from which you may need to override parameters for your experiments:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="c1"># Configuration version is useful in migrating older configs to new ones</span>
<span class="nt">config_version</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">1.0</span>

<span class="c1"># Configuration for training</span>
<span class="nt">training</span><span class="p">:</span>
    <span class="c1"># Name of the trainer class used to define the training/evalution loop</span>
    <span class="nt">trainer</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">base_trainer</span>
    <span class="c1"># Seed to be used for training. -1 means random seed between 1 and 100000.</span>
    <span class="c1"># Either pass fixed through your config or command line arguments</span>
    <span class="c1"># Pass null to the seed if you don&#39;t want it seeded anyhow and</span>
    <span class="c1"># want to leave it to default</span>
    <span class="nt">seed</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">-1</span>
    <span class="c1"># Name of the experiment, will be used while saving checkpoints</span>
    <span class="c1"># and generating reports</span>
    <span class="nt">experiment_name</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">run</span>
    <span class="c1"># Maximum number of iterations the training will run</span>
    <span class="nt">max_updates</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">22000</span>
    <span class="c1"># Maximum epochs in case you don&#39;t want to use max_updates</span>
    <span class="c1"># Can be mixed with max iterations, so it will stop whichever is</span>
    <span class="c1"># completed first. Default: null means epochs won&#39;t be used</span>
    <span class="nt">max_epochs</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>

    <span class="c1"># After `log_interval` iterations, current iteration&#39;s training loss will be</span>
    <span class="c1"># reported. This will also report validation on a single batch from validation set</span>
    <span class="c1"># to provide an estimate on validation side</span>
    <span class="nt">log_interval</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">100</span>
    <span class="c1"># Level of logging, only logs which are &gt;= to current level will be logged</span>
    <span class="nt">logger_level</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">info</span>
    <span class="c1"># Log format: json, simple</span>
    <span class="nt">log_format</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">simple</span>
    <span class="c1"># Whether to log detailed final configuration parameters</span>
    <span class="nt">log_detailed_config</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">false</span>
    <span class="c1"># Whether MMF should log or not, Default: False, which means</span>
    <span class="c1"># mmf will log by default</span>
    <span class="nt">should_not_log</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">false</span>

    <span class="c1"># Tensorboard control, by default tensorboard is disabled</span>
    <span class="nt">tensorboard</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">false</span>

    <span class="c1"># Size of each batch. If distributed or data_parallel</span>
    <span class="c1"># is used, this will be divided equally among GPUs</span>
    <span class="nt">batch_size</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">512</span>
    <span class="c1"># Number of workers to be used in dataloaders</span>
    <span class="nt">num_workers</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">4</span>
    <span class="c1"># Some datasets allow fast reading by loading everything in the memory</span>
    <span class="c1"># Use this to enable it</span>
    <span class="nt">fast_read</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">false</span>
    <span class="c1"># Use in multi-tasking, when you want to sample tasks proportional to their sizes</span>
    <span class="nt">dataset_size_proportional_sampling</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">true</span>
    <span class="c1"># Whether to pin memory in dataloader</span>
    <span class="nt">pin_memory</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">false</span>

    <span class="c1"># After `checkpoint_interval` iterations, MMF will make a snapshot</span>
    <span class="c1"># which will involve creating a checkpoint for current training scenarios</span>
    <span class="nt">checkpoint_interval</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">1000</span>
    <span class="c1"># This will evaluate evaluation metrics on whole validation set after</span>
    <span class="c1"># evaluation interval</span>
    <span class="nt">evaluation_interval</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">1000</span>
    <span class="c1"># Whether gradients should be clipped</span>
    <span class="nt">clip_gradients</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">false</span>
    <span class="c1"># Mode for clip norm</span>
    <span class="nt">clip_norm_mode</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">all</span>

    <span class="nt">early_stop</span><span class="p">:</span>
        <span class="c1"># Whether to use early stopping, (Default: false)</span>
        <span class="nt">enabled</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">false</span>
        <span class="c1"># Patience for early stoppings</span>
        <span class="nt">patience</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">4000</span>
        <span class="c1"># Criteria to be monitored for early stopping</span>
        <span class="c1"># total_loss will monitor combined loss from all of the tasks</span>
        <span class="c1"># Criteria can also be an evaluation metric in this format `dataset/metric`</span>
        <span class="c1"># for e.g. vqa2/vqa_accuracy</span>
        <span class="nt">criteria</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">total_loss</span>
        <span class="c1"># Whether the monitored criteria should be minimized for early stopping</span>
        <span class="c1"># or not, for e.g. you would want to minimize loss but maximize an evaluation</span>
        <span class="c1"># metric like accuracy etc.</span>
        <span class="nt">minimize</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">true</span>

    <span class="c1"># Should a lr scheduler be used</span>
    <span class="nt">lr_scheduler</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">false</span>

    <span class="c1"># DEPRECATED: Look at scheduler_attributes or</span>
    <span class="c1"># Use PythiaScheduler directly instead</span>
    <span class="c1"># Steps for LR scheduler, will be an array of iteration count</span>
    <span class="c1"># when lr should be decreased</span>
    <span class="nt">lr_steps</span><span class="p">:</span> <span class="p p-Indicator">[]</span>
    <span class="c1"># DEPRECATED: Look at scheduler_attributes or</span>
    <span class="c1"># Use PythiaScheduler directly instead</span>
    <span class="c1"># Ratio for each lr step</span>
    <span class="nt">lr_ratio</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">0.1</span>

    <span class="c1"># NOTE: Have a look at newer scheduler available in MMF (such as AdamW) before</span>
    <span class="c1"># using these options</span>
    <span class="c1"># Should use warmup for lr</span>
    <span class="nt">use_warmup</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">false</span>
    <span class="c1"># Warmup factor learning rate warmup</span>
    <span class="nt">warmup_factor</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">0.2</span>
    <span class="c1"># Iteration until which warnup should be done</span>
    <span class="nt">warmup_iterations</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">1000</span>

    <span class="c1"># Device on which the model will be trained. Set &#39;cpu&#39; to train/infer on CPU</span>
    <span class="nt">device</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">cuda</span>
    <span class="c1"># Local rank of the GPU device</span>
    <span class="nt">local_rank</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>

    <span class="c1"># If verbose dump is active, MMF will dump dataset, model specific</span>
    <span class="c1"># information which can be useful in debugging</span>
    <span class="nt">verbose_dump</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">false</span>

    <span class="c1"># Turn on if you want to ignore unused parameters in case of DDP</span>
    <span class="nt">find_unused_parameters</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">false</span>

    <span class="c1"># By default metrics evaluation is turned off during training. Set this to true</span>
    <span class="c1"># to enable evaluation every log_interval</span>
    <span class="nt">evaluate_metrics</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">false</span>

<span class="c1"># Configuration for evaluation</span>
<span class="nt">evaluation</span><span class="p">:</span>
    <span class="c1"># Metrics for evaluation</span>
    <span class="nt">metrics</span><span class="p">:</span> <span class="p p-Indicator">[]</span>
    <span class="c1"># Generate predictions in a file</span>
    <span class="nt">predict</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">false</span>
    <span class="c1"># Prediction file format (csv|json), default is json</span>
    <span class="nt">predict_file_format</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">json</span>

<span class="c1"># Configuration for models, default configuration files for various models</span>
<span class="c1"># included in MMF can be found under configs directory in root folder</span>
<span class="nt">model_config</span><span class="p">:</span> <span class="p p-Indicator">{}</span>

<span class="c1"># Configuration for datasets. Separate configuration</span>
<span class="c1"># for different datasets included in MMF are included in dataset folder</span>
<span class="c1"># which can be mixed and matched to train multiple datasets together</span>
<span class="c1"># An example for mixing all vqa datasets is present under vqa folder</span>
<span class="nt">dataset_config</span><span class="p">:</span> <span class="p p-Indicator">{}</span>

<span class="c1"># Defines which datasets from the above tasks you want to train on</span>
<span class="nt">datasets</span><span class="p">:</span> <span class="p p-Indicator">[]</span>

<span class="c1"># Defines which model you want to train on</span>
<span class="nt">model</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>

<span class="c1"># Config file to be optionally passed by the user</span>
<span class="nt">config</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>

<span class="c1"># Type of run, train+inference by default means both training and inference</span>
<span class="c1"># (test) stage will be run, if run_type contains &#39;val&#39;,</span>
<span class="c1"># inference will be run on val set also.</span>
<span class="nt">run_type</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">train_inference</span>

<span class="c1"># Configuration for optimizer, examples can be found in models&#39; configs in</span>
<span class="c1"># configs folder</span>
<span class="nt">optimizer</span><span class="p">:</span> <span class="p p-Indicator">{}</span>

<span class="c1"># Configuration for scheduler, examples can be found in models&#39; configs</span>
<span class="nt">scheduler</span><span class="p">:</span> <span class="p p-Indicator">{}</span>

<span class="c1"># Common environment configurations for MMF</span>
<span class="nt">env</span><span class="p">:</span>
    <span class="c1"># Universal cache directory for mmf</span>
    <span class="c1"># This can be overridden by using MMF_CACHE_DIR environment variable</span>
    <span class="c1"># or by directly setting this configuration attribute env.cache_dir</span>
    <span class="c1"># If nothing is specified, default is set to &quot;mmf&quot; inside</span>
    <span class="c1"># pytorch&#39;s cache folder</span>
    <span class="nt">cache_dir</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">${resolve_cache_dir:MMF_CACHE_DIR}</span>

    <span class="c1"># Config path for dataset zoo, can be overridden via environment</span>
    <span class="c1"># variable MMF_DATASET_ZOO as well.</span>
    <span class="nt">dataset_zoo</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">${env:MMF_DATASET_ZOO,configs/zoo/datasets.yaml}</span>
    <span class="nt">model_zoo</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">${env:MMF_MODEL_ZOO, configs/zoo/models.yaml}</span>

    <span class="c1"># Similar to cache dir, but can be used if specifically want to override</span>
    <span class="c1"># where MMF stores your data. Default would be cache_dir/data.</span>
    <span class="c1"># We will auto download models and datasets in this folder</span>
    <span class="nt">data_dir</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">${resolve_dir:MMF_DATA_DIR, data}</span>

    <span class="c1"># Directory for saving checkpoints and other metadata</span>
    <span class="c1"># Use MMF_SAVE_DIR or env.save_dir to override</span>
    <span class="nt">save_dir</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">${env:MMF_SAVE_DIR, ./save}</span>

    <span class="c1"># Directory for saving logs, default is &quot;logs&quot; inside the save folder</span>
    <span class="c1"># If log_dir is specifically passed, logs will be written inside that folder</span>
    <span class="c1"># Use MMF_LOG_DIR or env.log_dir to override</span>
    <span class="nt">log_dir</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">${env:MMF_LOG_DIR,}</span>

    <span class="c1"># Directory for saving reports, if not passed a opts based folder will be generated</span>
    <span class="c1"># inside save_dir/reports and reports will be saved there</span>
    <span class="c1"># Use MMF_REPORT_DIR or env.report_dir to override</span>
    <span class="nt">report_dir</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">${env:MMF_REPORT_DIR,}</span>

    <span class="c1"># Log directory for tensorboard, default points to same as logs</span>
    <span class="c1"># Only used when training.tensorboard is enabled.</span>
    <span class="c1"># Use MMF_TENSORBOARD_LOGDIR or env.tensorboard_logdir to override</span>
    <span class="nt">tensorboard_logdir</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">${env:MMF_TENSORBOARD_LOGDIR,}</span>

    <span class="c1"># User directory where user can keep their own models independent of MMF</span>
    <span class="c1"># This allows users to create projects which only include MMF as dependency</span>
    <span class="c1"># Use MMF_USER_DIR or env.user_dir to specify</span>
    <span class="nt">user_dir</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">${env:MMF_USER_DIR,}</span>

<span class="c1">###</span>
<span class="c1"># Configuration for the distributed setup</span>
<span class="nt">distributed</span><span class="p">:</span>
    <span class="c1">###</span>
    <span class="c1"># Typically tcp://hostname:port that will be used to establish initial connection</span>
    <span class="nt">init_method</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
    <span class="c1"># Rank of the current worker</span>
    <span class="nt">rank</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">0</span>
    <span class="c1"># Port number, not required if using init_method,</span>
    <span class="nt">port</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">-1</span>
    <span class="c1"># Backend for distributed setup</span>
    <span class="nt">backend</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">nccl</span>
    <span class="c1"># Total number of GPUs across all nodes (default: all visible GPUs)</span>
    <span class="nt">world_size</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">${device_count:}</span>
    <span class="c1"># Set if you do not want spawn multiple processes even if</span>
    <span class="c1"># multiple GPUs are visible</span>
    <span class="nt">no_spawn</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">false</span>

<span class="c1"># Configuration for checkpointing including resuming and loading pretrained models</span>
<span class="nt">checkpoint</span><span class="p">:</span>
    <span class="c1"># If checkpoint.resume is true, MMF will try to load automatically load</span>
    <span class="c1"># checkpoint and state from &quot;current.ckpt&quot; from env.save_dir</span>
    <span class="nt">resume</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">false</span>
    <span class="c1"># `checkpoint.resume_file` can be used to load a specific checkpoint from a file</span>
    <span class="c1"># Can also be a zoo key</span>
    <span class="nt">resume_file</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
    <span class="c1"># `checkpoint.resume_best` will load the best checkpoint according to</span>
    <span class="c1"># training.early_stop.criteria instead of the last saved ckpt</span>
    <span class="nt">resume_best</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">false</span>
    <span class="c1"># `checkpoint.resume_pretrained` can be used in conjuction with `resume_file`</span>
    <span class="c1"># or `resume_zoo` where you specify a checkpoint or .pth file to be loaded</span>
    <span class="c1"># but it is mapped based on `checkpoint.pretrained_state_mapping`</span>
    <span class="c1"># For e.g. if you want to resume from visual_bert pretrained on coco</span>
    <span class="c1"># You would set `checkpoint.resume_zoo=visual_bert.pretrained.coco` and</span>
    <span class="c1"># then set `checkpoint.resume_pretrained=True` which will then pick up</span>
    <span class="c1"># only the base which you would define in the `checkpoint.pretrained_state_mapping`</span>
    <span class="nt">resume_pretrained</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">false</span>
    <span class="c1"># `checkpoint.resume_zoo` can be used to resume from a pretrained model provided</span>
    <span class="c1"># in zoo. Value maps to key from zoo. `checkpoint.resume_file` has higher</span>
    <span class="c1"># priority compared to `checkpoint.resume_zoo`.</span>
    <span class="nt">resume_zoo</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
    <span class="c1"># `checkpoint.zoo_config_override` will override the current model config of trainer</span>
    <span class="c1"># with what is provided from the zoo checkpoint and will load the model</span>
    <span class="c1"># using .from_pretrained of the model passed</span>
    <span class="nt">zoo_config_override</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">false</span>
    <span class="c1"># `checkpoint.pretrained_state_mapping` specifies how exactly a pretrained</span>
    <span class="c1"># model will be loaded and mapped to which keys of the target model</span>
    <span class="c1"># Only use if the keys on the model in which pretrained model is to be loaded</span>
    <span class="c1"># don&#39;t match with those of the pretrained model or you only want to load specific</span>
    <span class="c1"># item from the pretrained model. `checkpoint.resume_pretrained` must be</span>
    <span class="c1"># true to use this mapping. for e.g. you can specify</span>
    <span class="c1"># text_embedding: text_embedding_pythia</span>
    <span class="c1"># for loading `text_embedding` module of your model from `text_embedding_pythia`of</span>
    <span class="c1"># pretrained file specified in `checkpoint.resume_file`.</span>
    <span class="nt">pretrained_state_mapping</span><span class="p">:</span> <span class="p p-Indicator">{}</span>

    <span class="c1"># Whether to save git details or not</span>
    <span class="nt">save_git_details</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">true</span>

    <span class="c1"># `checkpoint.reset` configuration defines what exactly should be reset</span>
    <span class="c1"># in case the file from which we are resuming is .ckpt and not .pth</span>
    <span class="nt">reset</span><span class="p">:</span>
        <span class="c1"># Everything will be reset except the state_dict of model</span>
        <span class="nt">all</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">false</span>
        <span class="c1"># Optimizer specifically will be reset</span>
        <span class="nt">optimizer</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">false</span>
        <span class="c1"># All counts such as best_update, current_iteration etc will be reset</span>
        <span class="nt">counts</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">false</span>
</pre></div>
</div>
</div>
</div>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="challenge.html" class="btn btn-neutral float-right" title="Challenge Participation" accesskey="n" rel="next">Next <img src="../_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="pretrained_models.html" class="btn btn-neutral" title="Pretrained Models" accesskey="p" rel="prev"><img src="../_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, Facebook AI Research.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">Configuration System</a><ul>
<li><a class="reference internal" href="#omegaconf">OmegaConf</a></li>
<li><a class="reference internal" href="#hierarchy">Hierarchy</a></li>
<li><a class="reference internal" href="#base-defaults">Base Defaults</a></li>
<li><a class="reference internal" href="#dataset-config">Dataset Config</a></li>
<li><a class="reference internal" href="#model-config">Model Config</a></li>
<li><a class="reference internal" href="#user-config">User Config</a></li>
<li><a class="reference internal" href="#command-line-dot-list-override">Command Line Dot List Override</a></li>
<li><a class="reference internal" href="#includes">Includes</a></li>
<li><a class="reference internal" href="#other-overrides">Other overrides</a></li>
<li><a class="reference internal" href="#environment-variables">Environment Variables</a></li>
<li><a class="reference internal" href="#base-defaults-config">Base Defaults Config</a></li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
         <script src="../_static/jquery.js"></script>
         <script src="../_static/underscore.js"></script>
         <script src="../_static/doctools.js"></script>
         <script src="../_static/language_data.js"></script>
     

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->


  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://mmf.sh/" class="footer-logo"></a>
      </div>
      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://mmf.sh/">MMF</a></li>
            <li><a href="https://mmf.sh/get-started">Get Started</a></li>
            <li><a href="https://mmf.sh/features">Features</a></li>
            <li><a href="https://github.com/facebookresearch/master/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="">Resources</a></li>
            <li><a href="https://mmf.sh/tutorials">Tutorials</a></li>
            <li><a href="https://mmf.sh/docs/stable/index.html">Docs</a></li>
            <li><a href="https://github.com/facebookresearch/master/issues" target="_blank">Github Issues</a></li>
          </ul>
        </div>
        </div>
      </div>
    </div>
  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://mmf.sh/" aria-label="MMF"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://mmf.sh/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://mmf.sh/features">Features</a>
          </li>

          <li>
            <a href="https://mmf.sh/tutorials">Tutorials</a>
          </li>

          <li class="active">
            <a href="https://mmf.sh/docs/stable/index.html">Docs</a>
          </li>
          <li>
            <a href="https://github.com/facebookresearch/mmf">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>