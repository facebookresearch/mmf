# Configuration version is useful in migrating older configs to new ones
config_version: 1.0

# Configuration for training
training:
    # Name of the trainer class used to define the training/evalution loop
    # `mmf` for default trainer, `lightning` for pytorch lightning trainer
    # pytorch lightning trainer's params is at `trainer.params`
    trainer: mmf
    # Seed to be used for training. -1 means random seed between 1 and 100000.
    # Either pass fixed through your config or command line arguments
    # Pass null to the seed if you don't want it seeded anyhow and
    # want to leave it to default
    seed: -1
    # Name of the experiment, will be used while saving checkpoints
    # and generating reports
    experiment_name: run
    # Maximum number of iterations the training will run
    max_updates: 22000
    # Maximum epochs in case you don't want to use max_updates
    # Can be mixed with max iterations, so it will stop whichever is
    # completed first. Default: null means epochs won't be used
    max_epochs: null

    # After `log_interval` updates, current update's training loss will be
    # reported. This will also report validation on a single batch from validation set
    # to provide an estimate on validation side
    log_interval: 100
    # Level of logging, only logs which are >= to current level will be logged
    logger_level: info
    # Log format: json, simple
    log_format: simple
    # Whether to log detailed final configuration parameters
    log_detailed_config: false
    # Whether MMF should log or not, Default: False, which means
    # mmf will log by default
    should_not_log: false
    # Whether the colored logs should be used
    colored_logs: true

    # Tensorboard control, by default tensorboard is disabled
    tensorboard: false

    # Weights and Biases control, by default Weights and Biases (wandb) is disabled
    wandb:
        # Whether to use Weights and Biases Logger, (Default: false)
        enabled: false
        # An entity is a username or team name where you're sending runs.
        # This is necessary if you want to log your metrics to a team account. By default
        # it will log the run to your user account.
        entity: null
        # Project name to be used while logging the experiment with wandb
        project: mmf
        # Experiment/ run name to be used while logging the experiment
        # under the project with wandb
        name: ${training.experiment_name}
        # You can save your model checkpoints as W&B Artifacts for model versioning.
        # Set the value to `true` to enable this feature.
        log_checkpoint: false
        # Specify other argument values that you want to pass to wandb.init(). Check out the documentation
        # at https://docs.wandb.ai/ref/python/init to see what arguments are available.
        # job_type: 'train'
        # tags: ['tag1', 'tag2']



    # Size of the batch globally. If distributed or data_parallel
    # is used, this will be divided equally among GPUs
    batch_size: 512
    # Size of the batch per GPU. Use this if you want to specify batch size
    # per GPU instead of global batch size.
    # Mutually exclusive with training.batch_size.
    batch_size_per_device: null
    # Run update_frequency=K batches with batch_size=N, accumulate gradients, then do
    # one update (one optimizer step). The effect is a large effective batch size of
    # KxN (without incurring the memory overhead of setting batch_size to KxN).
    update_frequency: 1
    # Number of workers to be used in dataloaders
    num_workers: 4
    # Some datasets allow fast reading by loading everything in the memory
    # Use this to enable it
    fast_read: false
    # Use in multi-tasking, when you want to sample tasks proportional to their sizes
    dataset_size_proportional_sampling: true
    # Whether to pin memory in dataloader
    pin_memory: false
    # Whether to use persistent workers in dataloader
    # (only effective for PyTorch 1.8 or higher which supports persistent_workers)
    persistent_workers: true

    # After `checkpoint_interval` number of updates, MMF will make a snapshot
    # which will involve creating a checkpoint for current training scenarios
    checkpoint_interval: 1000
    # This will evaluate evaluation metrics on whole validation set after
    # evaluation interval number of updates
    evaluation_interval: 1000
    # Whether gradients should be clipped
    clip_gradients: false
    # Mode for clip norm
    clip_norm_mode: all

    early_stop:
        # Whether to use early stopping, (Default: false)
        enabled: false
        # Patience for early stoppings
        patience: 4000
        # Criteria to be monitored for early stopping
        # total_loss will monitor combined loss from all of the tasks
        # Criteria can also be an evaluation metric in this format `dataset/metric`
        # for e.g. vqa2/vqa_accuracy
        criteria: total_loss
        # Whether the monitored criteria should be minimized for early stopping
        # or not, for e.g. you would want to minimize loss but maximize an evaluation
        # metric like accuracy etc.
        minimize: true

    # Should a lr scheduler be used
    lr_scheduler: false

    # DEPRECATED: Look at scheduler_attributes or
    # Use PythiaScheduler directly instead
    # Steps for LR scheduler, will be an array of iteration count
    # when lr should be decreased
    lr_steps: []
    # DEPRECATED: Look at scheduler_attributes or
    # Use PythiaScheduler directly instead
    # Ratio for each lr step
    lr_ratio: 0.1

    # NOTE: Have a look at newer scheduler available in MMF (such as AdamW) before
    # using these options
    # Should use warmup for lr
    use_warmup: false
    # Warmup factor learning rate warmup
    warmup_factor: 0.2
    # Iteration until which warnup should be done
    warmup_iterations: 1000

    # Device on which the model will be trained. Set 'cpu' to train/infer on CPU
    device: cuda
    # Local rank of the GPU device
    local_rank: null

    # If verbose dump is active, MMF will dump dataset, model specific
    # information which can be useful in debugging
    verbose_dump: false

    # Turn on if you want to ignore unused parameters in case of DDP
    find_unused_parameters: false

    # By default metrics evaluation is turned off during training. Set this to true
    # to enable evaluation every log_interval number of updates
    evaluate_metrics: false

    # This will enable anomaly detection mode in PyTorch. Use this for debugging
    # purposes if you see NaN issues in your experiments.
    # Warning: As per PyTorch docs, this usually slows down your code and should
    # only be used for debugging purposes
    detect_anomaly: false

    # FP16 support through torch.cuda.amp autocast and grad scaler.
    # Set to true to activate fp16 for faster performance with negligible
    # drop in results.
    fp16: false

    # Users can define their own callback functions in the trainer, e.g. adjust
    # learning rate, plot data in tensorboard, etc.
    # The format should look like:
    # callbacks:
    #   - type: my_callback
    #     params:
    #       foo: bar
    callbacks: []

    # check for NaNs in losses during training
    # Set to true to look for NaNs in the losses and exit the training when NaN happens
    exit_on_nan_losses: true

trainer:
    # Name of the trainer class used to define the training/evalution loop
    # `mmf` or `lightning` to specify the trainer to be used
    # `mmf` for mmf trainer,
    # for mmf trainer params, please see training params in the `training` config
    # `lightning` for Pytorch Lightning trainer
    # for lightning trainer params, please see lightning doc for details: ie.,
    # https://pytorch-lightning.readthedocs.io/en/latest/trainer.html#trainer-class-api
    type: lightning
    params:
        gpus: 1
        num_nodes: 1
        precision: 32
        deterministic: false
        benchmark: false
        max_steps: 22000
        max_epochs: null
        gradient_clip_val: 0.0
        num_sanity_val_steps: 0
        checkpoint_callback: true
        accumulate_grad_batches: 1
        val_check_interval: 1000
        log_every_n_steps: 100
        logger: false
        limit_val_batches: 1.0
        # progress bar is turned off if want same logging format as `mmf_trainer`
        progress_bar_refresh_rate: 0
        # `config.trainer.params.resume_from_checkpoint` always takes precedence over
        # `config.checkpoint` such as `config.checkpoint.resume_file`,
        # `config.checkpoint.resume_zoo` and `config.checkpoint.resume`.
        # `resume_from_checkpoint` is used to specify trainer resume parameters,
        # including resuming model state.
        # If `resume_from_checkpoint` is not specified, then we default to using
        # "current.ckpt" if `config.checkpoint.resume is True`, or "best.ckpt" if
        # `config.checkpoint.resume_best is True`. Else, we read it from
        # `config.checkpoint.resume_file` or `config.checkpoint.resume_zoo`
        # (in that order). If `resume_from_checkpoint` is not specified, and if
        # `config.checkpoint` is not specified, then no checkpoint file will be loaded.
        resume_from_checkpoint: null

# Configuration for evaluation
evaluation:
    # Metrics for evaluation
    metrics: []
    # Use CPU for metrics and other calculations, you can use this option if
    # you see OOM in validation as in metrics are calculated globally
    use_cpu: false
    # Generate predictions in a file
    predict: false
    # Prediction file format (csv|json), default is json
    predict_file_format: json
    # Test reporter params. Defaults to type: file
    reporter:
        type: file
        params: {}

# Configuration for models, default configuration files for various models
# included in MMF can be found under configs directory in root folder
model_config: {}

# Configuration for datasets. Separate configuration
# for different datasets included in MMF are included in dataset folder
# which can be mixed and matched to train multiple datasets together
# An example for mixing all vqa datasets is present under vqa folder
dataset_config: {}

# Defines which datasets from the above tasks you want to train on
datasets: []

# Defines which model you want to train on
model: null

# Config file to be optionally passed by the user
config: null

# Type of run, train+inference by default means both training and inference
# (test) stage will be run, if run_type contains 'val',
# inference will be run on val set also.
run_type: train_inference

# Configuration for optimizer, examples can be found in models' configs in
# configs folder
optimizer:
    # Whether to allow some of the model's parameters not to be used by the
    # optimizer. Default is false to guard against missing parameters due to
    # implementation errors in a model's get_optimizer_parameters
    allow_unused_parameters: false
    # Whether to enable optimizer state sharding. It uses ZeRO optimizer state
    # sharding method as described here https://arxiv.org/abs/1910.02054.
    enable_state_sharding: false

# Configuration for scheduler, examples can be found in models' configs
scheduler: {}

# Common environment configurations for MMF
env:
    # Universal cache directory for mmf
    # This can be overridden by using MMF_CACHE_DIR environment variable
    # or by directly setting this configuration attribute env.cache_dir
    # If nothing is specified, default is set to "mmf" inside
    # pytorch's cache folder
    cache_dir: ${resolve_cache_dir:MMF_CACHE_DIR}

    # Config path for dataset zoo, can be overridden via environment
    # variable MMF_DATASET_ZOO as well.
    dataset_zoo: ${env:MMF_DATASET_ZOO,configs/zoo/datasets.yaml}
    model_zoo: ${env:MMF_MODEL_ZOO, configs/zoo/models.yaml}

    # Similar to cache dir, but can be used if specifically want to override
    # where MMF stores your data. Default would be cache_dir/data.
    # We will auto download models and datasets in this folder
    data_dir: ${resolve_dir:MMF_DATA_DIR, data}

    # Directory for saving checkpoints and other metadata
    # Use MMF_SAVE_DIR or env.save_dir to override
    save_dir: ${env:MMF_SAVE_DIR, ./save}

    # Directory for saving logs, default is "logs" inside the save folder
    # If log_dir is specifically passed, logs will be written inside that folder
    # Use MMF_LOG_DIR or env.log_dir to override
    log_dir: ${env:MMF_LOG_DIR,}

    # Directory for saving reports, if not passed a opts based folder will be generated
    # inside save_dir/reports and reports will be saved there
    # Use MMF_REPORT_DIR or env.report_dir to override
    report_dir: ${env:MMF_REPORT_DIR,}

    # Log directory for tensorboard, default points to same as logs
    # Only used when training.tensorboard is enabled.
    # Use MMF_TENSORBOARD_LOGDIR or env.tensorboard_logdir to override
    tensorboard_logdir: ${env:MMF_TENSORBOARD_LOGDIR,}

    # Log directory for Weights and Biases, default points to same as logs
    # Only used when training.wandb is enabled.
    # Use MMF_WANDB_LOGDIR or env.wandb_logdir to override
    wandb_logdir: ${env:MMF_WANDB_LOGDIR,}

    # User directory where user can keep their own models independent of MMF
    # This allows users to create projects which only include MMF as dependency
    # Use MMF_USER_DIR or env.user_dir to specify
    user_dir: ${env:MMF_USER_DIR,}

###
# Configuration for the distributed setup
distributed:
    ###
    # Typically tcp://hostname:port that will be used to establish initial connection
    init_method: null
    # Rank of the current worker
    rank: 0
    # Port number, not required if using init_method,
    port: -1
    # Backend for distributed setup
    backend: nccl
    # Total number of GPUs across all nodes (default: all visible GPUs)
    world_size: ${device_count:}
    # Set if you do not want spawn multiple processes even if
    # multiple GPUs are visible
    no_spawn: false

# Configuration for checkpointing including resuming and loading pretrained models
checkpoint:
    # If checkpoint.resume is true or 1, MMF will try to load automatically load
    # checkpoint and state from "current.ckpt" from env.save_dir
    resume: false
    # `checkpoint.resume_file` can be used to load a specific checkpoint from a file
    # Can also be a zoo key
    resume_file: null
    # `checkpoint.resume_best` will load the best checkpoint according to
    # training.early_stop.criteria instead of the last saved ckpt
    resume_best: false
    # `checkpoint.resume_pretrained` can be used in conjuction with `resume_file`
    # or `resume_zoo` where you specify a checkpoint or .pth file to be loaded
    # but it is mapped based on `checkpoint.pretrained_state_mapping`
    # For e.g. if you want to resume from visual_bert pretrained on coco
    # You would set `checkpoint.resume_zoo=visual_bert.pretrained.coco` and
    # then set `checkpoint.resume_pretrained=True` which will then pick up
    # only the base which you would define in the `checkpoint.pretrained_state_mapping`
    resume_pretrained: false
    # `checkpoint.resume_zoo` can be used to resume from a pretrained model provided
    # in zoo. Value maps to key from zoo. `checkpoint.resume_file` has higher
    # priority compared to `checkpoint.resume_zoo`.
    resume_zoo: null
    # `checkpoint.zoo_config_override` will override the current model config of trainer
    # with what is provided from the zoo checkpoint and will load the model
    # using .from_pretrained of the model passed
    zoo_config_override: false
    # `checkpoint.pretrained_state_mapping` specifies how exactly a pretrained
    # model will be loaded and mapped to which keys of the target model
    # Only use if the keys on the model in which pretrained model is to be loaded
    # don't match with those of the pretrained model or you only want to load specific
    # item from the pretrained model. `checkpoint.resume_pretrained` must be
    # true to use this mapping. for e.g. you can specify
    # text_embedding: text_embedding_pythia
    # for loading `text_embedding` module of your model from `text_embedding_pythia`of
    # pretrained file specified in `checkpoint.resume_file`.
    pretrained_state_mapping: {}

    # Will save only the last max_to_keep; if -1, saves all eligible checkpoints
    max_to_keep: -1

    # Whether to save git details or not
    save_git_details: true

    # `checkpoint.reset` configuration defines what exactly should be reset
    # in case the file from which we are resuming is .ckpt and not .pth
    reset:
        # Everything will be reset except the state_dict of model
        all: false
        # Optimizer specifically will be reset
        optimizer: false
        # All counts such as best_update, current_iteration etc will be reset
        counts: false
        # If fp16 scaler should be reset or not
        fp16_scaler: false


multitasking:
    enabled: true
    type: size_proportional
    params: {}
