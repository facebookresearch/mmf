dataset_config:
  masked_vqa2:
    data_dir: ${env.data_dir}/datasets
    depth_first: false
    fast_read: false
    use_images: false
    add_answer: false
    use_features: true
    zoo_requirements:
    - coco.defaults
    - vqa2.defaults
    features:
      train:
      - coco/defaults/features/trainval2014.lmdb
      val:
      - coco/defaults/features/trainval2014.lmdb
      test:
      - coco/defaults/features/test2015.lmdb
    annotations:
      train:
      - vqa2/defaults/annotations/imdb_train2014.npy
      val:
      - vqa2/defaults/annotations/imdb_val2014.npy
      test:
      - vqa2/defaults/annotations/imdb_test2015.npy
    max_features: 100
    use_image_feature_masks: false
    processors:
      masked_token_processor:
        type: masked_token
        params:
          tokenizer_config:
            type: bert-base-uncased
            params:
              do_lower_case: true
          mask_probability: 0.15
          max_seq_length: 128
      answer_processor:
        type: vqa_answer
        params:
          num_answers: 10
          vocab_file: vqa2/defaults/extras/vocabs/answers_vqa.txt
          preprocessor:
            type: simple_word
            params: {}
      masked_region_processor:
        type: masked_region
        params:
          mask_probability: 0.15
          mask_region_probability: 0.90
      transformer_bbox_processor:
          type: transformer_bbox
          params:
            bbox_key: bbox
            image_width_key: image_width
            image_height_key: image_height
    return_features_info: false
    # Return OCR information
    use_ocr: false
    # Return spatial information of OCR tokens if present
    use_ocr_info: false
