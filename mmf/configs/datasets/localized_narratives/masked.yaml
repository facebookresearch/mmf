dataset_config:
  masked_localized_narratives:
    data_dir: ${env.data_dir}/datasets
    use_images: False
    use_features: True
    zoo_requirements:
    - localized_narratives.defaults
    - localized_narratives.only_captions
    - ade20k.defaults
    - coco.defaults
    - flickr30k.defaults
    features:
      train:
      - flickr30k/defaults/features/detectron.lmdb
      - ade20k/defaults/features/ade20k_train.lmdb
      - coco/defaults/features/coco_train2017.lmdb
      - localized_narratives/defaults/features/ln_open_images_train.lmdb
      val:
      - flickr30k/defaults/features/detectron.lmdb
      - ade20k/defaults/features/ade20k_val.lmdb
      - localized_narratives/defaults/features/open_images_val.lmdb
      - coco/defaults/features/coco_val2017.lmdb
      test:
      - flickr30k/defaults/features/detectron.lmdb
      - localized_narratives/defaults/features/open_images_test.lmdb
    annotations:
      train:
      - localized_narratives/defaults/annotations/flickr30k_train_captions.jsonl
      - localized_narratives/defaults/annotations/ade20k_train_captions.jsonl
      - localized_narratives/defaults/annotations/coco_train_captions.jsonl
      - localized_narratives/defaults/annotations/open_images_train_v6_captions.jsonl
      val:
      - localized_narratives/defaults/annotations/flickr30k_val_captions.jsonl
      - localized_narratives/defaults/annotations/ade20k_validation_captions.jsonl
      - localized_narratives/defaults/annotations/open_images_validation_captions.jsonl
      - localized_narratives/defaults/annotations/coco_val_captions.jsonl
      test:
      - localized_narratives/defaults/annotations/flickr30k_test_captions.jsonl
      - localized_narratives/defaults/annotations/open_images_test_captions.jsonl
    processors:
      masked_token_processor:
        type: masked_token
        params:
          tokenizer_config:
            type: bert-base-uncased
            params:
              do_lower_case: True
          mask_probability: 0.15
          max_seq_length: 64
      image_processor:
        type: torchvision_transforms
        params:
          transforms:
          - type: Resize
            params:
                size: [256, 256]
          - type: CenterCrop
            params:
              size: [224, 224]
          - ToTensor
          - GrayScaleTo3Channels
          - type: Normalize
            params:
              mean: [0.485, 0.456, 0.406]
              std: [0.229, 0.224, 0.225]
