dataset_config:
  vqacp_v2:
    data_dir: ${env.data_dir}/datasets
    use_images: true
    use_features: false
    zoo_requirements:
    - coco.defaults
    - vqa2.defaults
    - vqacp_v2.defaults
    images:
      train:
      - coco/defaults/images/train2014,coco/defaults/images/val2014
      val:
      - coco/defaults/images/train2014,coco/defaults/images/val2014
      test:
      - coco/defaults/images/train2014,coco/defaults/images/val2014
    annotations:
      train:
      - vqacp_v2/defaults/annotations/vqacp_v2_train_annotations.json,vqacp_v2/defaults/annotations/vqacp_v2_train_questions.json
      val:
      - vqacp_v2/defaults/annotations/vqacp_v2_test_annotations.json,vqacp_v2/defaults/annotations/vqacp_v2_test_questions.json
      test:
      - vqacp_v2/defaults/annotations/vqacp_v2_test_annotations.json,vqacp_v2/defaults/annotations/vqacp_v2_test_questions.json
    processors:
      text_processor:
        type: vocab
        params:
          max_length: 14
          vocab:
            type: intersected
            embedding_name: glove.6B.300d
            vocab_file: vqa2/defaults/extras/vocabs/vocabulary_100k.txt
          preprocessor:
            type: simple_sentence
            params: {}
      answer_processor:
        type: vqa_answer
        params:
          num_answers: 10
          vocab_file: vqa2/defaults/extras/vocabs/answers_vqa.txt
          preprocessor:
            type: simple_word
            params: {}
      image_processor:
        type: torchvision_transforms
        params:
          transforms:
            - type: Resize
              params:
                size: [256, 256]
            - type: CenterCrop
              params:
                size: [224, 224]
            - ToTensor
            - GrayScaleTo3Channels
            - type: Normalize
              params:
                mean: [0.485, 0.456, 0.406]
                std: [0.12221994, 0.12145835, 0.14380469]
